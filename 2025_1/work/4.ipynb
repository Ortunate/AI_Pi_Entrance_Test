{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cc201c",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## 1. Env set up\n",
    "1. ... already set up\n",
    "---\n",
    "## 2. Work Sequentially\n",
    "- using Copilot(Gemini 2.5 pro) to help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17415901",
   "metadata": {},
   "source": [
    "#### a. Subtask 1\n",
    "(1) Word Embedding\n",
    "1. **Definition**: A technique in NLP mapping words or phrases to vectors of real numbers.\n",
    "**Purpose**: Captures semantic meaning and relationships between words.\n",
    "2. Traditional methods often involve very high-dimensional sparse vectors (e.g., one-hot encoding), resulting in inefficiencies and inability to capture semantic relationships. Word embeddings, on the other hand, use dense vectors in lower-dimensional spaces, allowing for better representation of word meanings and relationships.\n",
    "3. **Common Examples**:\n",
    "    - Word2Vec\n",
    "    1. uses neural networks to learn word associations from large corpora of text.\n",
    "    2. combine Skip-gram(predict contexts given a target word) and Continuous Bag of Words (CBOW)(predict target word given contexts) models.\n",
    "\n",
    "(2) Multi-head Self-Attention\n",
    "1. **Main Idea**: Allows the model to jointly attend to information from different representation subspaces at different positions. (a little like adopting stochastic beam search in attention mechanism)\n",
    "2. **Scaled Dot-Product Attention**:\n",
    "    $$\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "    $$\n",
    "    - `Q` (**Query**): A matrix representing a set of queries. In self-attention, this is a projection of the input sequence. Each query vector represents a word asking for attention from all other words.\n",
    "    - `K` (**Key**): A matrix representing a set of keys. This is another projection of the input sequence. Each key vector can be thought of as a \"label\" for a word, which is matched against the queries.\n",
    "    - `V` (**Value**): A matrix representing a set of values. This is a third projection of the input sequence. Each value vector contains the actual information of a word that should be passed on.\n",
    "    - `d_k`: The dimension of the key vectors (and query vectors). The scaling factor `sqrt(d_k)` is crucial. For large values of `d_k`, the dot products can grow very large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Scaling counteracts this effect, leading to more stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f20456",
   "metadata": {},
   "source": [
    "#### b. Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b2f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 128) (10, 8, 20, 20)\n",
      "[-91.96555916 -19.40983534 -32.99740866 113.35786088 138.22610441\n",
      "  81.21040905 -30.81003178  90.7098463  162.38724319 -40.72173619] [1.94810489e-189 3.21476597e-151 3.61314239e-103 4.96644350e-219\n",
      " 3.90604112e-173 3.46437823e-131 4.72245009e-077 2.66307289e-194\n",
      " 1.00000000e+000 5.17103825e-098]\n"
     ]
    }
   ],
   "source": [
    "# Subtask 2\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(114514)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"Compute the scaled dot-product attention.\n",
    "\n",
    "    Args:(np.array, np.array, np.array, np.array)\n",
    "        Q (np.array): Queries of shape (..., seq_len_q, depth)\n",
    "        K (np.array): Keys of shape (..., seq_len_k, depth)\n",
    "        V (np.array): Values of shape (..., seq_len_v, depth_v)\n",
    "        mask (np.array, optional): Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        (np.array, np.array)\n",
    "        output: Attention output of shape (..., seq_len_q, depth_v)\n",
    "        attention_weights: Attention weights of shape (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = np.matmul(Q, K.swapaxes(-2, -1)) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "    attention_weights = softmax(scores)\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "def multi_head_attention(embed_size, num_heads, input, mask=None):\n",
    "    \"\"\"Multi-head attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): Dimensionality of the input embeddings.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        input (np.array): Input tensor of shape (batch_size, seq_len, embed_size).\n",
    "        mask (np.array, optional): Float tensor with shape broadcastable to (batch_size, num_heads, seq_len, seq_len). Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        (np.array, np.array)\n",
    "        output: Multi-head attention output of shape (batch_size, seq_len, embed_size)\n",
    "        weights: Attention weights of shape (batch_size, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = input.shape\n",
    "    assert embed_size % num_heads == 0, \"embed_size must be divisible by num_heads\"\n",
    "    head_dim = embed_size // num_heads\n",
    "    \n",
    "    Wq = np.random.randn(embed_size, embed_size)\n",
    "    Wk = np.random.randn(embed_size, embed_size)\n",
    "    Wv = np.random.randn(embed_size, embed_size)\n",
    "    Wo = np.random.randn(embed_size, embed_size)\n",
    "    \n",
    "    # Linear projections\n",
    "    Q = np.matmul(input, Wq)\n",
    "    K = np.matmul(input, Wk)\n",
    "    V = np.matmul(input, Wv)\n",
    "    \n",
    "    # Reshape and transpose for multi-head attention\n",
    "    Q = Q.reshape(batch_size, seq_len, num_heads, head_dim).swapaxes(1, 2)\n",
    "    K = K.reshape(batch_size, seq_len, num_heads, head_dim).swapaxes(1, 2)\n",
    "    V = V.reshape(batch_size, seq_len, num_heads, head_dim).swapaxes(1, 2)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    attention_output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "    \n",
    "    # Concatenate heads and project\n",
    "    attention_output = attention_output.swapaxes(1, 2).reshape(batch_size, seq_len, embed_size)\n",
    "    \n",
    "    output = np.matmul(attention_output, Wo)\n",
    "\n",
    "    return output, weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 10\n",
    "    seq_len = 20\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    input = np.random.randn(batch_size, seq_len, embed_size) \n",
    "    output, weights = multi_head_attention(embed_size, num_heads, input)\n",
    "\t\n",
    "    print(output.shape, weights.shape)\n",
    "    print(output[0][0][:10], weights[0][0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3f924",
   "metadata": {},
   "source": [
    "#### c. Subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec10ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 128]) (10, 8, 20, 20)\n",
      "tensor([ 0.1218,  0.0743,  0.0391,  0.0172, -0.0705, -0.1443, -0.0310, -0.2916,\n",
      "        -0.1042, -0.0591], grad_fn=<SliceBackward0>) [1.94810489e-189 3.21476597e-151 3.61314239e-103 4.96644350e-219\n",
      " 3.90604112e-173 3.46437823e-131 4.72245009e-077 2.66307289e-194\n",
      " 1.00000000e+000 5.17103825e-098]\n"
     ]
    }
   ],
   "source": [
    "# Subtask 3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"基于PyTorch的多头自注意力实现\"\"\"\n",
    "    def __init__(self, embed_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_size % num_heads == 0, \"embed_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        # linear layers for Q, K, V and output\n",
    "        self.Wq = nn.Linear(embed_size, embed_size)\n",
    "        self.Wk = nn.Linear(embed_size, embed_size)\n",
    "        self.Wv = nn.Linear(embed_size, embed_size)\n",
    "        self.Wo = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.swapaxes(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\t\t\t\t\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "    \n",
    "        # linear projections\n",
    "        Q = self.Wq(query)\n",
    "        K = self.Wk(key)\n",
    "        V = self.Wv(value)\n",
    "        \n",
    "        # reshape and transpose\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # scaled dot-product attention\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # concatenate heads and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
    "        \n",
    "        output = self.Wo(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 构造测试输入（与任务2保持一致的形状）\n",
    "    batch_size = 10\n",
    "    seq_len = 20\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    \n",
    "    input_tensor = torch.randn(batch_size, seq_len, embed_size)\n",
    "    model = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "    # 执行自注意力计算（query=key=value）\n",
    "    output, attn_weights = model(input_tensor, input_tensor, input_tensor)\n",
    "\n",
    "    print(output.shape, weights.shape)\n",
    "    print(output[0][0][:10], weights[0][0][0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241ca21",
   "metadata": {},
   "source": [
    "#### d. Subtask 4 VIT\n",
    "##### 1. Serialization: 2D -> 1D vector for transformer input\n",
    "1. Image Patching: split original image into fixed-size patches.\n",
    "2. Flattening and Linear Projection: each patch is flattened into a 1D vector and projected into a higher-dimensional space using a linear layer.\n",
    "##### 2. Positional Encoding\n",
    "1. Learnable Positional Embeddings: before feeding the patch embeddings into the transformer, add positional embeddings in the form of learnable vectors. These vectors are randomly initialized and learned during training.\n",
    "##### 3. Advantages over Traditional CNNs\n",
    "1. Global Context: self-attention mechanism captures long-range dependencies and global context more effectively than the local receptive fields of CNNs.\n",
    "2. Flexibility: can handle varying input sizes and resolutions more easily than CNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
