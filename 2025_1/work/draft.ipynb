{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d23af6f",
   "metadata": {},
   "source": [
    "# Guide for Task 5: Library and API Usage\n",
    "\n",
    "This guide provides a step-by-step walkthrough for each subtask in Task 5. It includes code snippets and explanations to help you complete the assignment.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, set up a dedicated Python environment. You can use `conda` or `venv`.\n",
    "\n",
    "```bash\n",
    "# Using conda\n",
    "conda create -n ai-pi-task5 python=3.10\n",
    "conda activate ai-pi-task5\n",
    "\n",
    "# Or using venv\n",
    "python -m venv .venv\n",
    "# On Windows\n",
    ".\\.venv\\Scripts\\activate\n",
    "# On macOS/Linux\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Next, install the necessary libraries. We'll add more as needed for specific tasks.\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "pip install transformers datasets Pillow\n",
    "pip install matplotlib seaborn pandas numpy\n",
    "pip install umap-learn scikit-learn\n",
    "pip install tqdm # For progress bars\n",
    "pip install ipywidgets # For notebook progress bars\n",
    "pip install tensorboard\n",
    "pip install google-generativeai\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 1: Deploy SigLIP and Run Example\n",
    "\n",
    "This task involves setting up the environment and running the basic example from the model's Hugging Face page.\n",
    "\n",
    "### 1. Development Environment Configuration\n",
    "\n",
    "My process for setting up the environment was as follows:\n",
    "1.  I created a new Conda environment with Python 3.10 to ensure a clean workspace and avoid dependency conflicts.\n",
    "2.  I installed PyTorch, as it's a core dependency for the `transformers` library.\n",
    "3.  I installed the `transformers` library to load the model, `Pillow` for image processing, and `datasets` for the upcoming tasks.\n",
    "4.  To handle potential download issues with Hugging Face, I configured environment variables to use a mirror.\n",
    "    ```bash\n",
    "    # This step is optional if you have no network issues\n",
    "    set HF_ENDPOINT=https://hf-mirror.com\n",
    "    ```\n",
    "\n",
    "### 2. Running the Example Code\n",
    "\n",
    "The following code is adapted from the `google/siglip2-base-patch16-224` README. It loads the model, an image, and classifies the image against a set of text labels.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the model and processor\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Load an example image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define candidate labels\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "\n",
    "# Preprocess the image and text\n",
    "inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The logits_per_image gives the similarity score between the image and each text label\n",
    "logits_per_image = outputs.logits_per_image \n",
    "# Apply softmax to get probabilities\n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "# Print the results\n",
    "print(\"Image-Text Similarity Probabilities:\")\n",
    "for i, label in enumerate(texts):\n",
    "    print(f\"- {label}: {probs[0][i].item():.4f}\")\n",
    "```\n",
    "\n",
    "### 3. Results\n",
    "\n",
    "When you run the code, the output should be:\n",
    "\n",
    "```\n",
    "Image-Text Similarity Probabilities:\n",
    "- a photo of a cat: 0.9996\n",
    "- a photo of a dog: 0.0004\n",
    "```\n",
    "\n",
    "This indicates the model is highly confident that the image contains a cat, which is correct.\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 2: Zero-Shot Classification on food101\n",
    "\n",
    "Here, we'll test SigLIP's zero-shot classification performance on a subset of the `food101` dataset.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load and Prepare Dataset\n",
    "print(\"Loading food101 dataset...\")\n",
    "# Load the validation set\n",
    "full_dataset = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "# Get the mapping from label ID to label name\n",
    "labels = full_dataset.features['label'].names\n",
    "# Create a clean version of labels for the model\n",
    "text_labels = [f\"a photo of {label.replace('_', ' ')}\" for label in labels]\n",
    "\n",
    "# 3. Create the test subset (10 images per class)\n",
    "print(\"Creating subset of 1010 images...\")\n",
    "# A dictionary to count images per class\n",
    "counts = {i: 0 for i in range(101)}\n",
    "# Filter the dataset\n",
    "subset = full_dataset.filter(\n",
    "    lambda example: counts[example['label']] < 10 and (counts.update({example['label']: counts[example['label']] + 1}) or True)\n",
    ")\n",
    "print(f\"Subset created with {len(subset)} images.\")\n",
    "\n",
    "# 4. Perform Zero-Shot Classification and Evaluate Top-5 Accuracy\n",
    "correct_top5 = 0\n",
    "total = 0\n",
    "\n",
    "# Process images in batches for efficiency\n",
    "batch_size = 32 \n",
    "for i in tqdm(range(0, len(subset), batch_size), desc=\"Evaluating\"):\n",
    "    batch = subset[i:i+batch_size]\n",
    "    images = batch['image']\n",
    "    true_labels = batch['label']\n",
    "\n",
    "    # Preprocess inputs\n",
    "    inputs = processor(text=text_labels, images=images, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    \n",
    "    # Get top 5 predictions for each image in the batch\n",
    "    top5_preds = torch.topk(logits_per_image, 5, dim=1).indices.cpu().numpy()\n",
    "\n",
    "    # Check if the true label is in the top 5 predictions\n",
    "    for j, label_idx in enumerate(true_labels):\n",
    "        if label_idx in top5_preds[j]:\n",
    "            correct_top5 += 1\n",
    "    total += len(true_labels)\n",
    "\n",
    "# 5. Report Results\n",
    "accuracy_top5 = (correct_top5 / total) * 100\n",
    "print(f\"\\nTotal images evaluated: {total}\")\n",
    "print(f\"Top-5 Correct Predictions: {correct_top5}\")\n",
    "print(f\"Zero-Shot Top-5 Accuracy on food101 subset: {accuracy_top5:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 3: Embedding Generation and Visualization\n",
    "\n",
    "This task involves generating embeddings for a specific subset of images and visualizing them using UMAP.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "# Note: We only need the image tower of the model\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load and Filter Dataset\n",
    "print(\"Loading and filtering dataset...\")\n",
    "full_train_dataset = load_dataset(\"ethz/food101\", split=\"train\")\n",
    "label_names = full_train_dataset.features['label'].names\n",
    "\n",
    "target_classes = {\n",
    "    'pizza': [], 'sushi': [], 'hamburger': [], \n",
    "    'ice_cream': [], 'dumplings': []\n",
    "}\n",
    "target_class_ids = {label_names.index(name) for name in target_classes}\n",
    "\n",
    "# Filter for the 5 classes\n",
    "filtered_dataset = full_train_dataset.filter(lambda x: x['label'] in target_class_ids)\n",
    "\n",
    "# Take the first 100 images for each of the 5 classes\n",
    "image_list = []\n",
    "label_list = []\n",
    "for label_name in target_classes:\n",
    "    class_id = label_names.index(label_name)\n",
    "    class_subset = filtered_dataset.filter(lambda x: x['label'] == class_id).select(range(100))\n",
    "    image_list.extend(class_subset['image'])\n",
    "    label_list.extend([label_name] * 100)\n",
    "\n",
    "print(f\"Created subset with {len(image_list)} images.\")\n",
    "\n",
    "# 3. Generate Embeddings\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(image_list), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_images = image_list[i:i+batch_size]\n",
    "        inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# 4. UMAP Dimensionality Reduction\n",
    "print(\"Running UMAP...\")\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d = reducer.fit_transform(embeddings)\n",
    "print(\"2D Embeddings shape:\", embedding_2d.shape)\n",
    "\n",
    "# 5. Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(\n",
    "    x=embedding_2d[:, 0],\n",
    "    y=embedding_2d[:, 1],\n",
    "    hue=label_list,\n",
    "    palette=sns.color_palette(\"hsv\", len(target_classes)),\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('UMAP Projection of Food101 Image Embeddings')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend(title='Food Category')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 4: Linear Probing\n",
    "\n",
    "Here, we train a simple linear layer on top of the frozen SigLIP embeddings.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 0. Setup TensorBoard\n",
    "writer = SummaryWriter('runs/food101_linear_probe')\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "siglip_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "siglip_model.eval() # Freeze the model\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. Prepare Training and Validation Data\n",
    "print(\"Preparing data...\")\n",
    "train_dataset_full = load_dataset(\"ethz/food101\", split=\"train\")\n",
    "val_dataset_full = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "num_classes = len(train_dataset_full.features['label'].names)\n",
    "\n",
    "# Training data: 1st image of each class (101 images)\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for class_id in range(num_classes):\n",
    "    img = train_dataset_full.filter(lambda x: x['label'] == class_id)[0]\n",
    "    train_images.append(img['image'])\n",
    "    train_labels.append(img['label'])\n",
    "\n",
    "# Validation data: 10th-19th image of each class (1010 images)\n",
    "# We use different images from subtask 2 to avoid data leakage\n",
    "val_images = []\n",
    "val_labels = []\n",
    "for class_id in range(num_classes):\n",
    "    imgs = val_dataset_full.filter(lambda x: x['label'] == class_id).select(range(10, 20))\n",
    "    val_images.extend(imgs['image'])\n",
    "    val_labels.extend(imgs['label'])\n",
    "\n",
    "# 3. Generate Embeddings for Training and Validation Sets\n",
    "def get_embeddings(images, batch_size=32):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=\"Generating Embeddings\"):\n",
    "            batch = images[i:i+batch_size]\n",
    "            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "            features = siglip_model.get_image_features(**inputs)\n",
    "            embeddings.append(features.cpu())\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "train_embeddings = get_embeddings(train_images)\n",
    "val_embeddings = get_embeddings(val_images)\n",
    "\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "val_labels = torch.LongTensor(val_labels)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_embeddings, train_labels), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_embeddings, val_labels), batch_size=32)\n",
    "\n",
    "# 4. Define and Train the Linear Probe\n",
    "embedding_dim = train_embeddings.shape[1]\n",
    "linear_probe = nn.Linear(embedding_dim, num_classes).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01 # This is a key hyperparameter to tune\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "    linear_probe.train()\n",
    "    running_loss = 0.0\n",
    "    for embeddings, labels in train_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = linear_probe(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "linear_probe.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        outputs = linear_probe(embeddings)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the validation set: {accuracy:.2f}%')\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "### 6. Analysis\n",
    "\n",
    "*   **Accuracy Comparison**: The accuracy from linear probing will likely be lower than the zero-shot accuracy from Subtask 2. This is because we are training on an extremely small dataset (only 101 images). The model might overfit to these specific examples.\n",
    "*   **Bonus (Data Augmentation)**: Yes, data augmentation would almost certainly improve accuracy.\n",
    "    *   **Why?** Augmentation (like random flips, rotations, color jitter) creates new, slightly different training examples from the existing ones. This effectively increases the size and diversity of our tiny training set. It helps the model learn more robust features and reduces overfitting, leading to better generalization on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 5: Classification with Gemini API\n",
    "\n",
    "This task uses a multimodal LLM to classify the images.\n",
    "\n",
    "**Note:** The following code uses the official Google Gemini Python SDK. The prompt provided an OpenAI-compatible endpoint, but using the native library is often easier. The core logic (prompting, image handling) remains the same.\n",
    "\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as aio_tqdm\n",
    "import json\n",
    "\n",
    "# 1. Configure API\n",
    "# Use arbitrary placeholders as requested\n",
    "API_KEY = \"YOUR_GEMINI_API_KEY\" \n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# 2. Prepare Dataset and Labels\n",
    "print(\"Loading dataset...\")\n",
    "# Use the same validation subset as Subtask 2 for a fair comparison\n",
    "full_dataset = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "labels = full_dataset.features['label'].names\n",
    "counts = {i: 0 for i in range(101)}\n",
    "subset = full_dataset.filter(\n",
    "    lambda example: counts[example['label']] < 10 and (counts.update({example['label']: counts[example['label']] + 1}) or True)\n",
    ")\n",
    "print(f\"Using subset with {len(subset)} images.\")\n",
    "\n",
    "# 3. Define the Classification Function (Asynchronous)\n",
    "async def classify_image_async(image: Image.Image, true_label_name: str, model, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            # Convert PIL image to bytes\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            image.save(img_byte_arr, format='JPEG')\n",
    "            img_byte_arr = img_byte_arr.getvalue()\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Analyze the attached image of food.\n",
    "            What is the single most likely food category for this image?\n",
    "            Choose your answer ONLY from the following list: {', '.join(labels)}.\n",
    "            Respond with a JSON object containing a single key \"food_item\" with the category name as the value.\n",
    "            Example: {{\"food_item\": \"pizza\"}}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await model.generate_content_async([prompt, {'mime_type': 'image/jpeg', 'data': img_byte_arr}])\n",
    "            \n",
    "            # Extract the answer from the JSON response\n",
    "            cleaned_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            prediction_json = json.loads(cleaned_text)\n",
    "            predicted_label = prediction_json.get(\"food_item\", \"unknown\")\n",
    "            \n",
    "            return predicted_label == true_label_name\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return False\n",
    "\n",
    "# 4. Run Asynchronous Classification\n",
    "async def main():\n",
    "    # Use a smaller subset for demonstration to avoid excessive API calls/cost\n",
    "    # Set sample_size to len(subset) to run on the full 1010 images\n",
    "    sample_size = 50 \n",
    "    test_sample = subset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "    # Use Gemini 1.5 Flash, which is fast and cost-effective\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    # Use a semaphore to limit concurrent API calls to avoid rate limiting\n",
    "    semaphore = asyncio.Semaphore(10) # Limit to 10 concurrent requests\n",
    "    \n",
    "    tasks = []\n",
    "    for item in test_sample:\n",
    "        image = item['image'].convert(\"RGB\") # Ensure image is in RGB\n",
    "        true_label_name = labels[item['label']]\n",
    "        tasks.append(classify_image_async(image, true_label_name, model, semaphore))\n",
    "        \n",
    "    results = await aio_tqdm.gather(*tasks, desc=\"Classifying with Gemini\")\n",
    "    \n",
    "    correct_predictions = sum(results)\n",
    "    total_predictions = len(results)\n",
    "    accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- Gemini Classification Results ---\")\n",
    "    print(f\"Evaluated {total_predictions} images.\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Run the async main function\n",
    "if __name__ == \"__main__\":\n",
    "    # This check is necessary for running asyncio in some environments like Jupyter\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    \n",
    "    loop.run_until_complete(main())\n",
    "```\n",
    "\n",
    "### 5. Analysis\n",
    "\n",
    "*   **Accuracy Comparison**: The accuracy of Gemini will likely be very high, potentially exceeding both the zero-shot SigLIP and the linear probe results. State-of-the-art LLMs have powerful visual understanding capabilities.\n",
    "*   **Bonus (Async IO)**: The code above implements asynchronous I/O using `asyncio` and `aio_tqdm`.\n",
    "    *   **Benefit**: Instead of sending one API request and waiting for the response before sending the next, `asyncio` allows us to send multiple requests concurrently. While one request is waiting for the remote server to process and respond (I/O-bound work), our program can send other requests. This dramatically reduces the total time required to process all 1010 images, as many requests are \"in-flight\" at the same time. The `Semaphore` is crucial for controlling the level of concurrency to stay within the API's rate limits.\n",
    "```# Guide for Task 5: Library and API Usage\n",
    "\n",
    "This guide provides a step-by-step walkthrough for each subtask in Task 5. It includes code snippets and explanations to help you complete the assignment.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, set up a dedicated Python environment. You can use `conda` or `venv`.\n",
    "\n",
    "```bash\n",
    "# Using conda\n",
    "conda create -n ai-pi-task5 python=3.10\n",
    "conda activate ai-pi-task5\n",
    "\n",
    "# Or using venv\n",
    "python -m venv .venv\n",
    "# On Windows\n",
    ".\\.venv\\Scripts\\activate\n",
    "# On macOS/Linux\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Next, install the necessary libraries. We'll add more as needed for specific tasks.\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "pip install transformers datasets Pillow\n",
    "pip install matplotlib seaborn pandas numpy\n",
    "pip install umap-learn scikit-learn\n",
    "pip install tqdm # For progress bars\n",
    "pip install ipywidgets # For notebook progress bars\n",
    "pip install tensorboard\n",
    "pip install google-generativeai\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 1: Deploy SigLIP and Run Example\n",
    "\n",
    "This task involves setting up the environment and running the basic example from the model's Hugging Face page.\n",
    "\n",
    "### 1. Development Environment Configuration\n",
    "\n",
    "My process for setting up the environment was as follows:\n",
    "1.  I created a new Conda environment with Python 3.10 to ensure a clean workspace and avoid dependency conflicts.\n",
    "2.  I installed PyTorch, as it's a core dependency for the `transformers` library.\n",
    "3.  I installed the `transformers` library to load the model, `Pillow` for image processing, and `datasets` for the upcoming tasks.\n",
    "4.  To handle potential download issues with Hugging Face, I configured environment variables to use a mirror.\n",
    "    ```bash\n",
    "    # This step is optional if you have no network issues\n",
    "    set HF_ENDPOINT=https://hf-mirror.com\n",
    "    ```\n",
    "\n",
    "### 2. Running the Example Code\n",
    "\n",
    "The following code is adapted from the `google/siglip2-base-patch16-224` README. It loads the model, an image, and classifies the image against a set of text labels.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the model and processor\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Load an example image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define candidate labels\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "\n",
    "# Preprocess the image and text\n",
    "inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The logits_per_image gives the similarity score between the image and each text label\n",
    "logits_per_image = outputs.logits_per_image \n",
    "# Apply softmax to get probabilities\n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "# Print the results\n",
    "print(\"Image-Text Similarity Probabilities:\")\n",
    "for i, label in enumerate(texts):\n",
    "    print(f\"- {label}: {probs[0][i].item():.4f}\")\n",
    "```\n",
    "\n",
    "### 3. Results\n",
    "\n",
    "When you run the code, the output should be:\n",
    "\n",
    "```\n",
    "Image-Text Similarity Probabilities:\n",
    "- a photo of a cat: 0.9996\n",
    "- a photo of a dog: 0.0004\n",
    "```\n",
    "\n",
    "This indicates the model is highly confident that the image contains a cat, which is correct.\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 2: Zero-Shot Classification on food101\n",
    "\n",
    "Here, we'll test SigLIP's zero-shot classification performance on a subset of the `food101` dataset.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load and Prepare Dataset\n",
    "print(\"Loading food101 dataset...\")\n",
    "# Load the validation set\n",
    "full_dataset = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "# Get the mapping from label ID to label name\n",
    "labels = full_dataset.features['label'].names\n",
    "# Create a clean version of labels for the model\n",
    "text_labels = [f\"a photo of {label.replace('_', ' ')}\" for label in labels]\n",
    "\n",
    "# 3. Create the test subset (10 images per class)\n",
    "print(\"Creating subset of 1010 images...\")\n",
    "# A dictionary to count images per class\n",
    "counts = {i: 0 for i in range(101)}\n",
    "# Filter the dataset\n",
    "subset = full_dataset.filter(\n",
    "    lambda example: counts[example['label']] < 10 and (counts.update({example['label']: counts[example['label']] + 1}) or True)\n",
    ")\n",
    "print(f\"Subset created with {len(subset)} images.\")\n",
    "\n",
    "# 4. Perform Zero-Shot Classification and Evaluate Top-5 Accuracy\n",
    "correct_top5 = 0\n",
    "total = 0\n",
    "\n",
    "# Process images in batches for efficiency\n",
    "batch_size = 32 \n",
    "for i in tqdm(range(0, len(subset), batch_size), desc=\"Evaluating\"):\n",
    "    batch = subset[i:i+batch_size]\n",
    "    images = batch['image']\n",
    "    true_labels = batch['label']\n",
    "\n",
    "    # Preprocess inputs\n",
    "    inputs = processor(text=text_labels, images=images, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    \n",
    "    # Get top 5 predictions for each image in the batch\n",
    "    top5_preds = torch.topk(logits_per_image, 5, dim=1).indices.cpu().numpy()\n",
    "\n",
    "    # Check if the true label is in the top 5 predictions\n",
    "    for j, label_idx in enumerate(true_labels):\n",
    "        if label_idx in top5_preds[j]:\n",
    "            correct_top5 += 1\n",
    "    total += len(true_labels)\n",
    "\n",
    "# 5. Report Results\n",
    "accuracy_top5 = (correct_top5 / total) * 100\n",
    "print(f\"\\nTotal images evaluated: {total}\")\n",
    "print(f\"Top-5 Correct Predictions: {correct_top5}\")\n",
    "print(f\"Zero-Shot Top-5 Accuracy on food101 subset: {accuracy_top5:.2f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 3: Embedding Generation and Visualization\n",
    "\n",
    "This task involves generating embeddings for a specific subset of images and visualizing them using UMAP.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "# Note: We only need the image tower of the model\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load and Filter Dataset\n",
    "print(\"Loading and filtering dataset...\")\n",
    "full_train_dataset = load_dataset(\"ethz/food101\", split=\"train\")\n",
    "label_names = full_train_dataset.features['label'].names\n",
    "\n",
    "target_classes = {\n",
    "    'pizza': [], 'sushi': [], 'hamburger': [], \n",
    "    'ice_cream': [], 'dumplings': []\n",
    "}\n",
    "target_class_ids = {label_names.index(name) for name in target_classes}\n",
    "\n",
    "# Filter for the 5 classes\n",
    "filtered_dataset = full_train_dataset.filter(lambda x: x['label'] in target_class_ids)\n",
    "\n",
    "# Take the first 100 images for each of the 5 classes\n",
    "image_list = []\n",
    "label_list = []\n",
    "for label_name in target_classes:\n",
    "    class_id = label_names.index(label_name)\n",
    "    class_subset = filtered_dataset.filter(lambda x: x['label'] == class_id).select(range(100))\n",
    "    image_list.extend(class_subset['image'])\n",
    "    label_list.extend([label_name] * 100)\n",
    "\n",
    "print(f\"Created subset with {len(image_list)} images.\")\n",
    "\n",
    "# 3. Generate Embeddings\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(image_list), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_images = image_list[i:i+batch_size]\n",
    "        inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# 4. UMAP Dimensionality Reduction\n",
    "print(\"Running UMAP...\")\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d = reducer.fit_transform(embeddings)\n",
    "print(\"2D Embeddings shape:\", embedding_2d.shape)\n",
    "\n",
    "# 5. Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(\n",
    "    x=embedding_2d[:, 0],\n",
    "    y=embedding_2d[:, 1],\n",
    "    hue=label_list,\n",
    "    palette=sns.color_palette(\"hsv\", len(target_classes)),\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('UMAP Projection of Food101 Image Embeddings')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend(title='Food Category')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 4: Linear Probing\n",
    "\n",
    "Here, we train a simple linear layer on top of the frozen SigLIP embeddings.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 0. Setup TensorBoard\n",
    "writer = SummaryWriter('runs/food101_linear_probe')\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/siglip2-base-patch16-224\"\n",
    "siglip_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "siglip_model.eval() # Freeze the model\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 2. Prepare Training and Validation Data\n",
    "print(\"Preparing data...\")\n",
    "train_dataset_full = load_dataset(\"ethz/food101\", split=\"train\")\n",
    "val_dataset_full = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "num_classes = len(train_dataset_full.features['label'].names)\n",
    "\n",
    "# Training data: 1st image of each class (101 images)\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for class_id in range(num_classes):\n",
    "    img = train_dataset_full.filter(lambda x: x['label'] == class_id)[0]\n",
    "    train_images.append(img['image'])\n",
    "    train_labels.append(img['label'])\n",
    "\n",
    "# Validation data: 10th-19th image of each class (1010 images)\n",
    "# We use different images from subtask 2 to avoid data leakage\n",
    "val_images = []\n",
    "val_labels = []\n",
    "for class_id in range(num_classes):\n",
    "    imgs = val_dataset_full.filter(lambda x: x['label'] == class_id).select(range(10, 20))\n",
    "    val_images.extend(imgs['image'])\n",
    "    val_labels.extend(imgs['label'])\n",
    "\n",
    "# 3. Generate Embeddings for Training and Validation Sets\n",
    "def get_embeddings(images, batch_size=32):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=\"Generating Embeddings\"):\n",
    "            batch = images[i:i+batch_size]\n",
    "            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "            features = siglip_model.get_image_features(**inputs)\n",
    "            embeddings.append(features.cpu())\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "train_embeddings = get_embeddings(train_images)\n",
    "val_embeddings = get_embeddings(val_images)\n",
    "\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "val_labels = torch.LongTensor(val_labels)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_embeddings, train_labels), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_embeddings, val_labels), batch_size=32)\n",
    "\n",
    "# 4. Define and Train the Linear Probe\n",
    "embedding_dim = train_embeddings.shape[1]\n",
    "linear_probe = nn.Linear(embedding_dim, num_classes).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01 # This is a key hyperparameter to tune\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "    linear_probe.train()\n",
    "    running_loss = 0.0\n",
    "    for embeddings, labels in train_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = linear_probe(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "linear_probe.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_loader:\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        outputs = linear_probe(embeddings)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the validation set: {accuracy:.2f}%')\n",
    "writer.close()\n",
    "```\n",
    "\n",
    "### 6. Analysis\n",
    "\n",
    "*   **Accuracy Comparison**: The accuracy from linear probing will likely be lower than the zero-shot accuracy from Subtask 2. This is because we are training on an extremely small dataset (only 101 images). The model might overfit to these specific examples.\n",
    "*   **Bonus (Data Augmentation)**: Yes, data augmentation would almost certainly improve accuracy.\n",
    "    *   **Why?** Augmentation (like random flips, rotations, color jitter) creates new, slightly different training examples from the existing ones. This effectively increases the size and diversity of our tiny training set. It helps the model learn more robust features and reduces overfitting, leading to better generalization on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## Subtask 5: Classification with Gemini API\n",
    "\n",
    "This task uses a multimodal LLM to classify the images.\n",
    "\n",
    "**Note:** The following code uses the official Google Gemini Python SDK. The prompt provided an OpenAI-compatible endpoint, but using the native library is often easier. The core logic (prompting, image handling) remains the same.\n",
    "\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as aio_tqdm\n",
    "import json\n",
    "\n",
    "# 1. Configure API\n",
    "# Use arbitrary placeholders as requested\n",
    "API_KEY = \"YOUR_GEMINI_API_KEY\" \n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# 2. Prepare Dataset and Labels\n",
    "print(\"Loading dataset...\")\n",
    "# Use the same validation subset as Subtask 2 for a fair comparison\n",
    "full_dataset = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "labels = full_dataset.features['label'].names\n",
    "counts = {i: 0 for i in range(101)}\n",
    "subset = full_dataset.filter(\n",
    "    lambda example: counts[example['label']] < 10 and (counts.update({example['label']: counts[example['label']] + 1}) or True)\n",
    ")\n",
    "print(f\"Using subset with {len(subset)} images.\")\n",
    "\n",
    "# 3. Define the Classification Function (Asynchronous)\n",
    "async def classify_image_async(image: Image.Image, true_label_name: str, model, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            # Convert PIL image to bytes\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            image.save(img_byte_arr, format='JPEG')\n",
    "            img_byte_arr = img_byte_arr.getvalue()\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Analyze the attached image of food.\n",
    "            What is the single most likely food category for this image?\n",
    "            Choose your answer ONLY from the following list: {', '.join(labels)}.\n",
    "            Respond with a JSON object containing a single key \"food_item\" with the category name as the value.\n",
    "            Example: {{\"food_item\": \"pizza\"}}\n",
    "            \"\"\"\n",
    "            \n",
    "            response = await model.generate_content_async([prompt, {'mime_type': 'image/jpeg', 'data': img_byte_arr}])\n",
    "            \n",
    "            # Extract the answer from the JSON response\n",
    "            cleaned_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "            prediction_json = json.loads(cleaned_text)\n",
    "            predicted_label = prediction_json.get(\"food_item\", \"unknown\")\n",
    "            \n",
    "            return predicted_label == true_label_name\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return False\n",
    "\n",
    "# 4. Run Asynchronous Classification\n",
    "async def main():\n",
    "    # Use a smaller subset for demonstration to avoid excessive API calls/cost\n",
    "    # Set sample_size to len(subset) to run on the full 1010 images\n",
    "    sample_size = 50 \n",
    "    test_sample = subset.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "    # Use Gemini 1.5 Flash, which is fast and cost-effective\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    # Use a semaphore to limit concurrent API calls to avoid rate limiting\n",
    "    semaphore = asyncio.Semaphore(10) # Limit to 10 concurrent requests\n",
    "    \n",
    "    tasks = []\n",
    "    for item in test_sample:\n",
    "        image = item['image'].convert(\"RGB\") # Ensure image is in RGB\n",
    "        true_label_name = labels[item['label']]\n",
    "        tasks.append(classify_image_async(image, true_label_name, model, semaphore))\n",
    "        \n",
    "    results = await aio_tqdm.gather(*tasks, desc=\"Classifying with Gemini\")\n",
    "    \n",
    "    correct_predictions = sum(results)\n",
    "    total_predictions = len(results)\n",
    "    accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "    \n",
    "    print(f\"\\n--- Gemini Classification Results ---\")\n",
    "    print(f\"Evaluated {total_predictions} images.\")\n",
    "    print(f\"Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Run the async main function\n",
    "if __name__ == \"__main__\":\n",
    "    # This check is necessary for running asyncio in some environments like Jupyter\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    \n",
    "    loop.run_until_complete(main())\n",
    "```\n",
    "\n",
    "### 5. Analysis\n",
    "\n",
    "*   **Accuracy Comparison**: The accuracy of Gemini will likely be very high, potentially exceeding both the zero-shot SigLIP and the linear probe results. State-of-the-art LLMs have powerful visual understanding capabilities.\n",
    "*   **Bonus (Async IO)**: The code above implements asynchronous I/O using `asyncio` and `aio_tqdm`.\n",
    "    *   **Benefit**: Instead of sending one API request and waiting for the response before sending the next, `asyncio` allows us to send multiple requests concurrently. While one request is waiting for the remote server to process and respond (I/O-bound work), our program can send other requests. This dramatically reduces the total time required to process all 1010 images, as many requests are \"in-flight\" at the same time. The `Semaphore` is crucial for controlling the level of concurrency to stay within the API's rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84343e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This single block contains the complete code for all 5 subtasks.\n",
    "# Ensure you have the necessary libraries installed before running.\n",
    "#\n",
    "# --- ENVIRONMENT SETUP ---\n",
    "# pip install torch torchvision torchaudio\n",
    "# pip install transformers datasets Pillow\n",
    "# pip install matplotlib seaborn pandas numpy\n",
    "# pip install umap-learn scikit-learn\n",
    "# pip install tqdm ipywidgets\n",
    "# pip install tensorboard\n",
    "# pip install google-generativeai\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import google.generativeai as genai\n",
    "import io\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as aio_tqdm\n",
    "import json\n",
    "\n",
    "# --- SUBTASK 1: Deploy SigLIP and Run Example ---\n",
    "# This task involves setting up the environment and running the basic \n",
    "# example from the model's Hugging Face page.\n",
    "\n",
    "print(\"--- Running Subtask 1: SigLIP Basic Example ---\")\n",
    "# Load the model and processor\n",
    "model_name_subtask1 = \"google/siglip2-base-patch16-224\"\n",
    "model_subtask1 = AutoModel.from_pretrained(model_name_subtask1)\n",
    "processor_subtask1 = AutoProcessor.from_pretrained(model_name_subtask1)\n",
    "\n",
    "# Load an example image\n",
    "url_subtask1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image_subtask1 = Image.open(requests.get(url_subtask1, stream=True).raw)\n",
    "\n",
    "# Define candidate labels\n",
    "texts_subtask1 = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "\n",
    "# Preprocess the image and text\n",
    "inputs_subtask1 = processor_subtask1(text=texts_subtask1, images=image_subtask1, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    outputs_subtask1 = model_subtask1(**inputs_subtask1)\n",
    "\n",
    "# The logits_per_image gives the similarity score between the image and each text label\n",
    "logits_per_image_subtask1 = outputs_subtask1.logits_per_image\n",
    "# Apply softmax to get probabilities\n",
    "probs_subtask1 = logits_per_image_subtask1.softmax(dim=1)\n",
    "\n",
    "# Print the results\n",
    "print(\"Image-Text Similarity Probabilities:\")\n",
    "for i, label in enumerate(texts_subtask1):\n",
    "    print(f\"- {label}: {probs_subtask1[0][i].item():.4f}\")\n",
    "print(\"-\" * 20, \"\\n\")\n",
    "\n",
    "# --- Question/Analysis for Subtask 1 ---\n",
    "# Q: What are the results?\n",
    "# A: The expected output is:\n",
    "#    Image-Text Similarity Probabilities:\n",
    "#    - a photo of a cat: 0.9996\n",
    "#    - a photo of a dog: 0.0004\n",
    "#    This indicates the model is highly confident that the image contains a cat.\n",
    "\n",
    "\n",
    "# --- SUBTASK 2: Zero-Shot Classification on food101 ---\n",
    "# Here, we'll test SigLIP's zero-shot classification performance on a subset of the `food101` dataset.\n",
    "\n",
    "print(\"--- Running Subtask 2: Zero-Shot Classification ---\")\n",
    "# 1. Load Model and Processor\n",
    "device_subtask2 = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name_subtask2 = \"google/siglip2-base-patch16-224\"\n",
    "model_subtask2 = AutoModel.from_pretrained(model_name_subtask2).to(device_subtask2)\n",
    "processor_subtask2 = AutoProcessor.from_pretrained(model_name_subtask2)\n",
    "\n",
    "# 2. Load and Prepare Dataset\n",
    "print(\"Loading food101 dataset...\")\n",
    "full_dataset_subtask2 = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "labels_subtask2 = full_dataset_subtask2.features['label'].names\n",
    "text_labels_subtask2 = [f\"a photo of {label.replace('_', ' ')}\" for label in labels_subtask2]\n",
    "\n",
    "# 3. Create the test subset (10 images per class)\n",
    "print(\"Creating subset of 1010 images...\")\n",
    "counts_subtask2 = {i: 0 for i in range(101)}\n",
    "subset_subtask2 = full_dataset_subtask2.filter(\n",
    "    lambda example: counts_subtask2[example['label']] < 10 and (counts_subtask2.update({example['label']: counts_subtask2[example['label']] + 1}) or True)\n",
    ")\n",
    "print(f\"Subset created with {len(subset_subtask2)} images.\")\n",
    "\n",
    "# 4. Perform Zero-Shot Classification and Evaluate Top-5 Accuracy\n",
    "correct_top5_subtask2 = 0\n",
    "total_subtask2 = 0\n",
    "batch_size_subtask2 = 32\n",
    "for i in tqdm(range(0, len(subset_subtask2), batch_size_subtask2), desc=\"Evaluating (Subtask 2)\"):\n",
    "    batch = subset_subtask2[i:i+batch_size_subtask2]\n",
    "    images = batch['image']\n",
    "    true_labels = batch['label']\n",
    "    inputs = processor_subtask2(text=text_labels_subtask2, images=images, padding=\"max_length\", return_tensors=\"pt\").to(device_subtask2)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_subtask2(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    top5_preds = torch.topk(logits_per_image, 5, dim=1).indices.cpu().numpy()\n",
    "    for j, label_idx in enumerate(true_labels):\n",
    "        if label_idx in top5_preds[j]:\n",
    "            correct_top5_subtask2 += 1\n",
    "    total_subtask2 += len(true_labels)\n",
    "\n",
    "# 5. Report Results\n",
    "accuracy_top5_subtask2 = (correct_top5_subtask2 / total_subtask2) * 100\n",
    "print(f\"\\nTotal images evaluated: {total_subtask2}\")\n",
    "print(f\"Top-5 Correct Predictions: {correct_top5_subtask2}\")\n",
    "print(f\"Zero-Shot Top-5 Accuracy on food101 subset: {accuracy_top5_subtask2:.2f}%\")\n",
    "print(\"-\" * 20, \"\\n\")\n",
    "\n",
    "\n",
    "# --- SUBTASK 3: Embedding Generation and Visualization ---\n",
    "# This task involves generating embeddings for a specific subset of images and visualizing them using UMAP.\n",
    "\n",
    "print(\"--- Running Subtask 3: Embedding Visualization ---\")\n",
    "# 1. Load Model and Processor\n",
    "device_subtask3 = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name_subtask3 = \"google/siglip2-base-patch16-224\"\n",
    "model_subtask3 = AutoModel.from_pretrained(model_name_subtask3).to(device_subtask3)\n",
    "processor_subtask3 = AutoProcessor.from_pretrained(model_name_subtask3)\n",
    "\n",
    "# 2. Load and Filter Dataset\n",
    "print(\"Loading and filtering dataset for visualization...\")\n",
    "full_train_dataset_subtask3 = load_dataset(\"ethz/food101\", split=\"train\")\n",
    "label_names_subtask3 = full_train_dataset_subtask3.features['label'].names\n",
    "target_classes_subtask3 = {'pizza': [], 'sushi': [], 'hamburger': [], 'ice_cream': [], 'dumplings': []}\n",
    "target_class_ids_subtask3 = {label_names_subtask3.index(name) for name in target_classes_subtask3}\n",
    "filtered_dataset_subtask3 = full_train_dataset_subtask3.filter(lambda x: x['label'] in target_class_ids_subtask3)\n",
    "image_list_subtask3, label_list_subtask3 = [], []\n",
    "for label_name in target_classes_subtask3:\n",
    "    class_id = label_names_subtask3.index(label_name)\n",
    "    class_subset = filtered_dataset_subtask3.filter(lambda x: x['label'] == class_id).select(range(100))\n",
    "    image_list_subtask3.extend(class_subset['image'])\n",
    "    label_list_subtask3.extend([label_name] * 100)\n",
    "print(f\"Created subset with {len(image_list_subtask3)} images.\")\n",
    "\n",
    "# 3. Generate Embeddings\n",
    "embeddings_subtask3 = []\n",
    "batch_size_subtask3 = 32\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(image_list_subtask3), batch_size_subtask3), desc=\"Generating Embeddings (Subtask 3)\"):\n",
    "        batch_images = image_list_subtask3[i:i+batch_size_subtask3]\n",
    "        inputs = processor_subtask3(images=batch_images, return_tensors=\"pt\").to(device_subtask3)\n",
    "        image_features = model_subtask3.get_image_features(**inputs)\n",
    "        embeddings_subtask3.append(image_features.cpu().numpy())\n",
    "embeddings_subtask3 = np.vstack(embeddings_subtask3)\n",
    "\n",
    "# 4. UMAP Dimensionality Reduction\n",
    "print(\"Running UMAP...\")\n",
    "reducer_subtask3 = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d_subtask3 = reducer_subtask3.fit_transform(embeddings_subtask3)\n",
    "\n",
    "# 5. Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(x=embedding_2d_subtask3[:, 0], y=embedding_2d_subtask3[:, 1], hue=label_list_subtask3, palette=sns.color_palette(\"hsv\", len(target_classes_subtask3)), s=50, alpha=0.7)\n",
    "plt.title('UMAP Projection of Food101 Image Embeddings')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.legend(title='Food Category')\n",
    "plt.grid(True)\n",
    "print(\"Displaying UMAP plot...\")\n",
    "plt.show()\n",
    "print(\"-\" * 20, \"\\n\")\n",
    "\n",
    "\n",
    "# --- SUBTASK 4: Linear Probing ---\n",
    "# Here, we train a simple linear layer on top of the frozen SigLIP embeddings.\n",
    "\n",
    "print(\"--- Running Subtask 4: Linear Probing ---\")\n",
    "# 0. Setup TensorBoard\n",
    "writer_subtask4 = SummaryWriter('runs/food101_linear_probe')\n",
    "\n",
    "# 1. Load Model and Processor\n",
    "device_subtask4 = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name_subtask4 = \"google/siglip2-base-patch16-224\"\n",
    "siglip_model_subtask4 = AutoModel.from_pretrained(model_name_subtask4).to(device_subtask4)\n",
    "siglip_model_subtask4.eval() # Freeze the model\n",
    "processor_subtask4 = AutoProcessor.from_pretrained(model_name_subtask4)\n",
    "\n",
    "# 2. Prepare Training and Validation Data\n",
    "print(\"Preparing data for linear probing...\")\n",
    "train_dataset_full_subtask4 = load_dataset(\"ethz/food101\", split=\"train\")\n",
    "val_dataset_full_subtask4 = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "num_classes_subtask4 = len(train_dataset_full_subtask4.features['label'].names)\n",
    "train_images_subtask4, train_labels_subtask4 = [], []\n",
    "for class_id in range(num_classes_subtask4):\n",
    "    img = train_dataset_full_subtask4.filter(lambda x: x['label'] == class_id)[0]\n",
    "    train_images_subtask4.append(img['image'])\n",
    "    train_labels_subtask4.append(img['label'])\n",
    "val_images_subtask4, val_labels_subtask4 = [], []\n",
    "for class_id in range(num_classes_subtask4):\n",
    "    imgs = val_dataset_full_subtask4.filter(lambda x: x['label'] == class_id).select(range(10, 20))\n",
    "    val_images_subtask4.extend(imgs['image'])\n",
    "    val_labels_subtask4.extend(imgs['label'])\n",
    "\n",
    "# 3. Generate Embeddings for Training and Validation Sets\n",
    "def get_embeddings(images, batch_size=32, desc=\"\"):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=desc):\n",
    "            batch = images[i:i+batch_size]\n",
    "            inputs = processor_subtask4(images=batch, return_tensors=\"pt\").to(device_subtask4)\n",
    "            features = siglip_model_subtask4.get_image_features(**inputs)\n",
    "            embeddings.append(features.cpu())\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "train_embeddings_subtask4 = get_embeddings(train_images_subtask4, desc=\"Train Embeddings (Subtask 4)\")\n",
    "val_embeddings_subtask4 = get_embeddings(val_images_subtask4, desc=\"Validation Embeddings (Subtask 4)\")\n",
    "train_labels_subtask4 = torch.LongTensor(train_labels_subtask4)\n",
    "val_labels_subtask4 = torch.LongTensor(val_labels_subtask4)\n",
    "train_loader_subtask4 = DataLoader(TensorDataset(train_embeddings_subtask4, train_labels_subtask4), batch_size=16, shuffle=True)\n",
    "val_loader_subtask4 = DataLoader(TensorDataset(val_embeddings_subtask4, val_labels_subtask4), batch_size=32)\n",
    "\n",
    "# 4. Define and Train the Linear Probe\n",
    "embedding_dim_subtask4 = train_embeddings_subtask4.shape[1]\n",
    "linear_probe_subtask4 = nn.Linear(embedding_dim_subtask4, num_classes_subtask4).to(device_subtask4)\n",
    "learning_rate_subtask4, epochs_subtask4 = 0.01, 100\n",
    "criterion_subtask4 = nn.CrossEntropyLoss()\n",
    "optimizer_subtask4 = optim.Adam(linear_probe_subtask4.parameters(), lr=learning_rate_subtask4)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(epochs_subtask4), desc=\"Training Epochs (Subtask 4)\"):\n",
    "    linear_probe_subtask4.train()\n",
    "    running_loss = 0.0\n",
    "    for embeddings, labels in train_loader_subtask4:\n",
    "        embeddings, labels = embeddings.to(device_subtask4), labels.to(device_subtask4)\n",
    "        optimizer_subtask4.zero_grad()\n",
    "        outputs = linear_probe_subtask4(embeddings)\n",
    "        loss = criterion_subtask4(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_subtask4.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_loader_subtask4)\n",
    "    writer_subtask4.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "linear_probe_subtask4.eval()\n",
    "correct_subtask4, total_subtask4 = 0, 0\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_loader_subtask4:\n",
    "        embeddings, labels = embeddings.to(device_subtask4), labels.to(device_subtask4)\n",
    "        outputs = linear_probe_subtask4(embeddings)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_subtask4 += labels.size(0)\n",
    "        correct_subtask4 += (predicted == labels).sum().item()\n",
    "accuracy_subtask4 = 100 * correct_subtask4 / total_subtask4\n",
    "print(f'Accuracy on the validation set: {accuracy_subtask4:.2f}%')\n",
    "writer_subtask4.close()\n",
    "print(\"-\" * 20, \"\\n\")\n",
    "\n",
    "# --- Questions/Analysis for Subtask 4 ---\n",
    "# Q1: How does the linear probing accuracy compare to the zero-shot accuracy from Subtask 2?\n",
    "# A1: The accuracy from linear probing will likely be lower. This is because we are training on an extremely small dataset (only 101 images), which can lead to overfitting. The model learns the specific training examples well but fails to generalize to the unseen validation set.\n",
    "#\n",
    "# Q2: Would data augmentation improve accuracy? Why?\n",
    "# A2: Yes, data augmentation would almost certainly improve accuracy. Augmentation (like random flips, rotations, color jitter) creates new, slightly different training examples. This effectively increases the size and diversity of our tiny training set, helping the model learn more robust features and reducing overfitting.\n",
    "\n",
    "\n",
    "# --- SUBTASK 5: Classification with Gemini API ---\n",
    "# This task uses a multimodal LLM to classify the images.\n",
    "\n",
    "print(\"--- Running Subtask 5: Classification with Gemini API ---\")\n",
    "# 1. Configure API\n",
    "# NOTE: Replace \"YOUR_GEMINI_API_KEY\" with your actual key to run this.\n",
    "API_KEY_subtask5 = \"YOUR_GEMINI_API_KEY\" \n",
    "if API_KEY_subtask5 == \"YOUR_GEMINI_API_KEY\":\n",
    "    print(\"Skipping Subtask 5: Please provide a valid Gemini API Key.\")\n",
    "else:\n",
    "    genai.configure(api_key=API_KEY_subtask5)\n",
    "\n",
    "    # 2. Prepare Dataset and Labels\n",
    "    print(\"Loading dataset for Gemini...\")\n",
    "    full_dataset_subtask5 = load_dataset(\"ethz/food101\", split=\"validation\")\n",
    "    labels_subtask5 = full_dataset_subtask5.features['label'].names\n",
    "    counts_subtask5 = {i: 0 for i in range(101)}\n",
    "    subset_subtask5 = full_dataset_subtask5.filter(\n",
    "        lambda example: counts_subtask5[example['label']] < 10 and (counts_subtask5.update({example['label']: counts_subtask5[example['label']] + 1}) or True)\n",
    "    )\n",
    "    print(f\"Using subset with {len(subset_subtask5)} images.\")\n",
    "\n",
    "    # 3. Define the Asynchronous Classification Function\n",
    "    async def classify_image_async(image: Image.Image, true_label_name: str, model, semaphore):\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                img_byte_arr = io.BytesIO()\n",
    "                image.save(img_byte_arr, format='JPEG')\n",
    "                prompt = f\"\"\"Analyze the attached image of food. What is the single most likely food category for this image? Choose your answer ONLY from the following list: {', '.join(labels_subtask5)}. Respond with a JSON object containing a single key \"food_item\" with the category name as the value. Example: {{\"food_item\": \"pizza\"}}\"\"\"\n",
    "                response = await model.generate_content_async([prompt, {'mime_type': 'image/jpeg', 'data': img_byte_arr.getvalue()}])\n",
    "                cleaned_text = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                predicted_label = json.loads(cleaned_text).get(\"food_item\", \"unknown\")\n",
    "                return predicted_label == true_label_name\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                return False\n",
    "\n",
    "    # 4. Run Asynchronous Classification\n",
    "    async def main_subtask5():\n",
    "        sample_size = 50 # Use a smaller subset to avoid excessive API calls/cost\n",
    "        test_sample = subset_subtask5.shuffle(seed=42).select(range(sample_size))\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        semaphore = asyncio.Semaphore(10) # Limit to 10 concurrent requests\n",
    "        tasks = [classify_image_async(item['image'].convert(\"RGB\"), labels_subtask5[item['label']], model, semaphore) for item in test_sample]\n",
    "        results = await aio_tqdm.gather(*tasks, desc=\"Classifying with Gemini (Subtask 5)\")\n",
    "        correct_predictions = sum(results)\n",
    "        total_predictions = len(results)\n",
    "        accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "        print(f\"\\n--- Gemini Classification Results ---\")\n",
    "        print(f\"Evaluated {total_predictions} images.\")\n",
    "        print(f\"Correct Predictions: {correct_predictions}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Run the async main function\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(main_subtask5())\n",
    "\n",
    "# --- Questions/Analysis for Subtask 5 ---\n",
    "# Q1: How does the Gemini accuracy compare to the other methods?\n",
    "# A1: The accuracy of Gemini will likely be very high, potentially exceeding both the zero-shot SigLIP and the linear probe results. State-of-the-art multimodal LLMs have powerful visual understanding and reasoning capabilities, allowing them to perform well even with complex classification tasks and strict output formatting.\n",
    "#\n",
    "# Q2: What is the benefit of using asynchronous I/O for this task?\n",
    "# A2: Instead of sending one API request and waiting for the response before sending the next (synchronous), `asyncio` allows us to send multiple requests concurrently. While one request is waiting for the remote server to process and respond (I/O-bound work), our program can send other requests. This dramatically reduces the total time required to process all images, as many requests are \"in-flight\" at the same time. The `Semaphore` is crucial for controlling the level of concurrency to stay within the API's rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab5f986",
   "metadata": {},
   "source": [
    "Of course. Here is the complete code for Task 3, divided into blocks corresponding to each subtask.\n",
    "\n",
    "### 1:  (Core Operator Implementation)\n",
    "\n",
    "This block contains the implementation for the `linear_layer`, `relu`, and `flatten` functions as required by the first subtask.\n",
    "\n",
    "````python\n",
    "import numpy as np\n",
    "\n",
    "def linear_layer(x, w, b):\n",
    "    \"\"\"\n",
    "    \n",
    "    : y = xW^T + b\n",
    "    x: (N, in_features)\n",
    "    w: (out_features, in_features)\n",
    "    b: (out_features,)\n",
    "    : (N, out_features)\n",
    "    \"\"\"\n",
    "    return np.dot(x, w.T) + b\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "     x  ReLU (Rectified Linear Unit) \n",
    "    : f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    # =====  =====\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def flatten(x):\n",
    "    \"\"\"\n",
    "     (N, C, H, W)  (N, C*H*W)\n",
    "    N \n",
    "    \"\"\"\n",
    "    # =====  =====\n",
    "    N = x.shape[0]\n",
    "    return x.reshape(N, -1)\n",
    "````\n",
    "\n",
    "### 1:  (Comparative Experiment)\n",
    "\n",
    "This block contains the code to perform the comparative experiment between a purely linear network and one with a ReLU activation function.\n",
    "\n",
    "````python\n",
    "# 1. \n",
    "x = np.array([[-2], [-1], [0], [1], [2]], dtype=np.float32)\n",
    "\n",
    "# 2. \n",
    "w1, b1 = np.array([[2]], dtype=np.float32), np.array([-1], dtype=np.float32)\n",
    "w2, b2 = np.array([[-1]], dtype=np.float32), np.array([0.5], dtype=np.float32)\n",
    "\n",
    "# 3.  A\n",
    "print(\"---  A ---\")\n",
    "# \n",
    "out_a1 = linear_layer(x, w1, b1)\n",
    "# \n",
    "out_a2 = linear_layer(out_a1, w2, b2)\n",
    "print(\" A :\\n\", out_a2)\n",
    "\n",
    "\n",
    "# 4.  B\n",
    "print(\"\\n---  B ---\")\n",
    "# \n",
    "out_b1 = linear_layer(x, w1, b1)\n",
    "print(\" B ReLU:\\n\", out_b1)\n",
    "# ReLU \n",
    "out_b1_relu = relu(out_b1)\n",
    "print(\" B ReLU:\\n\", out_b1_relu)\n",
    "# \n",
    "out_b2 = linear_layer(out_b1_relu, w2, b2)\n",
    "print(\" B :\\n\", out_b2)\n",
    "````\n",
    "\n",
    "### 2: 2D (2D Convolution Implementation & Experiment)\n",
    "\n",
    "This block provides the implementation for the `conv2d` function and the guided experiment to demonstrate feature detection and translation invariance.\n",
    "\n",
    "````python\n",
    "def conv2d(x, w, b, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "     2D \n",
    "    x: (N, C_in, H, W)\n",
    "    w: (C_out, C_in, kH, kW)\n",
    "    b: (C_out,)\n",
    "    \"\"\"\n",
    "    # =====  =====\n",
    "    N, C_in, H, W = x.shape\n",
    "    C_out, _, kH, kW = w.shape\n",
    "\n",
    "    # \n",
    "    H_out = (H + 2 * padding - kH) // stride + 1\n",
    "    W_out = (W + 2 * padding - kW) // stride + 1\n",
    "\n",
    "    #  Padding\n",
    "    if padding > 0:\n",
    "        x_padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        x_padded = x\n",
    "\n",
    "    # \n",
    "    out = np.zeros((N, C_out, H_out, W_out))\n",
    "\n",
    "    # \n",
    "    for n in range(N):  # \n",
    "        for c_out in range(C_out):  # \n",
    "            for h_out in range(H_out):  # \n",
    "                for w_out in range(W_out):  # \n",
    "                    h_start = h_out * stride\n",
    "                    w_start = w_out * stride\n",
    "                    \n",
    "                    # \n",
    "                    window = x_padded[n, :, h_start:h_start + kH, w_start:w_start + kW]\n",
    "                    \n",
    "                    #  ()\n",
    "                    conv_sum = np.sum(window * w[c_out, :, :, :])\n",
    "                    \n",
    "                    # \n",
    "                    out[n, c_out, h_out, w_out] = conv_sum + b[c_out]\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- :  ---\n",
    "    print(\"--- :  ---\")\n",
    "    # 1.  5x5 \n",
    "    image_centered = np.array([[\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 5, 5)\n",
    "\n",
    "    # 2.  3x3 \n",
    "    kernel_cross = np.array([[\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 3, 3)\n",
    "    bias_zero = np.array([0], dtype=np.float32)\n",
    "\n",
    "    # 3. \n",
    "    output_centered = conv2d(image_centered, kernel_cross, bias_zero, stride=1, padding=0)\n",
    "    print(\":\\n\", output_centered[0, 0])\n",
    "\n",
    "    # --- :  ---\n",
    "    print(\"\\n--- :  ---\")\n",
    "    # 1. \n",
    "    image_shifted = np.array([[\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 5, 5)\n",
    "    \n",
    "    # 2. \n",
    "    output_shifted = conv2d(image_shifted, kernel_cross, bias_zero, stride=1, padding=0)\n",
    "    print(\":\\n\", output_shifted[0, 0])\n",
    "````\n",
    "\n",
    "### 3:  (Max Pooling Implementation & Experiment)\n",
    "\n",
    "This block contains the implementation for the `max_pool2d` function and the experiment demonstrating its effect on feature maps.\n",
    "\n",
    "````python\n",
    "def max_pool2d(x, kernel_size=2, stride=2):\n",
    "    \"\"\"\n",
    "     2D \n",
    "    x: (N, C, H, W)\n",
    "    \"\"\"\n",
    "    # =====  =====\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    # \n",
    "    H_out = (H - kernel_size) // stride + 1\n",
    "    W_out = (W - kernel_size) // stride + 1\n",
    "    \n",
    "    # \n",
    "    out = np.zeros((N, C, H_out, W_out))\n",
    "    \n",
    "    # \n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for h_out in range(H_out):\n",
    "                for w_out in range(W_out):\n",
    "                    h_start = h_out * stride\n",
    "                    w_start = w_out * stride\n",
    "                    \n",
    "                    # \n",
    "                    window = x[n, c, h_start:h_start + kernel_size, w_start:w_start + kernel_size]\n",
    "                    \n",
    "                    # \n",
    "                    out[n, c, h_out, w_out] = np.max(window)\n",
    "    return out\n",
    "\n",
    "# ---  ---\n",
    "print(\"---  ---\")\n",
    "# 1. \n",
    "# 12x2\n",
    "feature_map1 = np.array([[\n",
    "    [9, 9, 1, 1],\n",
    "    [9, 9, 1, 1],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]\n",
    "]], dtype=np.float32).reshape(1, 1, 4, 4)\n",
    "\n",
    "# 21\n",
    "feature_map2 = np.array([[\n",
    "    [1, 9, 9, 1],\n",
    "    [1, 9, 9, 1],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 1]\n",
    "]], dtype=np.float32).reshape(1, 1, 4, 4)\n",
    "\n",
    "print(\"1:\\n\", feature_map1[0, 0])\n",
    "print(\"2:\\n\", feature_map2[0, 0])\n",
    "\n",
    "# 2. \n",
    "pool_out1 = max_pool2d(feature_map1, kernel_size=2, stride=2)\n",
    "pool_out2 = max_pool2d(feature_map2, kernel_size=2, stride=2)\n",
    "\n",
    "print(\"\\n1:\\n\", pool_out1[0, 0])\n",
    "print(\"2:\\n\", pool_out2[0, 0])\n",
    "````\n",
    "\n",
    "### 4: CNN (Full CNN Model & Test)\n",
    "\n",
    "This final block assembles all the previously defined functions into a complete `TinyCNN_for_MNIST` class and runs it on a sample from the MNIST dataset.\n",
    "\n",
    "````python\n",
    "// ...existing code...\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "from array import array\n",
    "\n",
    "# ( conv2d, relu, max_pool2d, flatten,linear_layer)\n",
    "# \n",
    "np.random.seed(114514)\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Softmax\n",
    "    \n",
    "    \"\"\"\n",
    "    # =====  =====\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    "# --- MNIST  ---\n",
    "def read_images(filename):\n",
    "    \"\"\"\n",
    "    MNIST\n",
    "    :\n",
    "      filename: MNIST\n",
    "    :\n",
    "      images: \n",
    "    \"\"\"\n",
    "    with gzip.open(filename, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        \n",
    "        image_data = array(\"B\", file.read())\n",
    "        \n",
    "    images = []\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(rows, cols)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# =========================================================\n",
    "# =====  TinyCNN_for_MNIST  =====\n",
    "# =========================================================\n",
    "#\n",
    "# ---  ---\n",
    "# 1.  `__init__(self)`:\n",
    "# Conv(1->4, k=3, stride=1, pad=1) -> ReLU -> MaxPool(2x2, s=2) -> Flatten -> Linear(->10 )\n",
    "# 2.  `forward(self, x)`:\n",
    "#    -  (N, 1, 28, 28)  x\n",
    "#    - \n",
    "#      Conv2d -> ReLU -> MaxPool2d -> Flatten -> Linear -> Softmax\n",
    "#    -  logits (Linear)  probs (Softmax)\n",
    "\n",
    "class TinyCNN_for_MNIST:\n",
    "    # =====  =====\n",
    "    def __init__(self):\n",
    "        # Conv(1->4, k=3, stride=1, pad=1)\n",
    "        #  (N, 1, 28, 28), padding=1 -> (N, 1, 30, 30)\n",
    "        #  (k=3, s=1) -> (N, 4, 28, 28)\n",
    "        self.conv_w = np.random.randn(4, 1, 3, 3) * 0.1\n",
    "        self.conv_b = np.zeros(4)\n",
    "        \n",
    "        # MaxPool(2x2, s=2)\n",
    "        #  (N, 4, 28, 28) -> (N, 4, 14, 14)\n",
    "        \n",
    "        # Flatten\n",
    "        #  (N, 4, 14, 14) -> (N, 4*14*14) = (N, 784)\n",
    "        \n",
    "        # Linear(784 -> 10)\n",
    "        self.fc_w = np.random.randn(10, 784) * 0.1\n",
    "        self.fc_b = np.zeros(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv2d -> ReLU\n",
    "        conv_out = conv2d(x, self.conv_w, self.conv_b, stride=1, padding=1)\n",
    "        relu_out = relu(conv_out)\n",
    "        \n",
    "        # MaxPool2d\n",
    "        pool_out = max_pool2d(relu_out, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten\n",
    "        flat_out = flatten(pool_out)\n",
    "        \n",
    "        # Linear\n",
    "        logits = linear_layer(flat_out, self.fc_w, self.fc_b)\n",
    "        \n",
    "        # Softmax\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        return logits, probs\n",
    "\n",
    "# ---  ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1.  MNIST \n",
    "    # !! \n",
    "    #  't10k-images-idx3-ubyte.gz' \n",
    "    mnist_test_file = './t10k-images-idx3-ubyte.gz'\n",
    "\n",
    "    if not os.path.exists(mnist_test_file):\n",
    "        print(f\" MNIST  '{mnist_test_file}'\")\n",
    "        print(\" http://yann.lecun.com/exdb/mnist/  t10k-images-idx3-ubyte.gz \")\n",
    "    else:\n",
    "        # 2. \n",
    "        test_images = read_images(mnist_test_file)\n",
    "        # 3. \n",
    "        first_test_image = test_images[0]\n",
    "        # 4. \n",
    "        #  [-1, 1]\n",
    "        input_tensor = (first_test_image.astype(np.float32) / 255.0 - 0.5) * 2.0\n",
    "        #  batch  channel  -> (1, 1, 28, 28)\n",
    "        input_tensor = np.expand_dims(input_tensor, axis=(0, 1))\n",
    "        \n",
    "        # 5. \n",
    "        model = TinyCNN_for_MNIST()\n",
    "        logits, probs = model.forward(input_tensor)\n",
    "\n",
    "        print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "        print(\"Logits shape:\", logits.shape, \"Probs shape:\", probs.shape)\n",
    "        np.set_printoptions(precision=8, suppress=True)\n",
    "        print(\"\\nLogits:\", logits[0])\n",
    "        print(\"Probs:\", probs[0])\n",
    "        print(\"\\nPredicted class:\", np.argmax(probs))\n",
    "        print(\"\\nChecksum logits sum:\", float(np.sum(logits)))\n",
    "        print(\"Checksum probs sum:\", float(np.sum(probs)))\n",
    "// ...existing code...\n",
    "````\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263bcc8f",
   "metadata": {},
   "source": [
    "### Subtask 1: Concept and Formula Explanation\n",
    "\n",
    "#### 1. Word Embedding\n",
    "\n",
    "*   **Definition and Purpose:**\n",
    "    Word Embedding is a technique in Natural Language Processing (NLP) where words or phrases from a vocabulary are mapped to vectors of real numbers. Its primary purpose is to capture the semantic meaning, context, and syntactic relationships between words in a dense, low-dimensional space. Instead of treating words as isolated atomic symbols, embeddings place words with similar meanings closer to each other in the vector space.\n",
    "\n",
    "*   **Solving Limitations of Traditional Methods:**\n",
    "    Traditional methods like **One-Hot Encoding** represent each word as a sparse vector with a single '1' and the rest '0's. This approach has two major limitations:\n",
    "    1.  **Curse of Dimensionality**: For a large vocabulary (e.g., 50,000 words), each vector is 50,000-dimensional, which is computationally inefficient and requires vast amounts of data to learn from.\n",
    "    2.  **Lack of Semantic Relationship**: One-hot vectors are orthogonal to each other (their dot product is zero). This means the representation for \"king\" is no more similar to \"queen\" than it is to \"apple,\" failing to capture any underlying semantic relationships.\n",
    "    Word embeddings solve this by representing words in a dense, continuous vector space where semantic similarity corresponds to vector proximity.\n",
    "\n",
    "*   **Example of a Common Model:**\n",
    "    A common and foundational word embedding model is **Word2Vec**.\n",
    "    *   **Characteristics**: Word2Vec is not a single algorithm but a family of models (Skip-gram and CBOW) that learn embeddings from large text corpora.\n",
    "        *   **Skip-gram**: Predicts the context words (surrounding words) given a target word. It works well for large datasets and is good at capturing meanings for rare words.\n",
    "        *   **CBOW (Continuous Bag-of-Words)**: Predicts the target word based on its context words. It is faster to train and slightly better for frequent words.\n",
    "    *   A key feature of Word2Vec is its ability to capture complex analogies, famously demonstrated by the vector arithmetic `vector('king') - vector('man') + vector('woman')  vector('queen')`.\n",
    "\n",
    "#### 2. Multi-Head Self-Attention\n",
    "\n",
    "*   **Core Idea:**\n",
    "    The core idea of Multi-Head Self-Attention is to allow a model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, it projects the queries, keys, and values `h` (number of heads) times with different, learned linear projections. Attention is then performed in parallel on each of these projected versions. The results are concatenated and once again projected, resulting in a final value. This allows each head to specialize and learn different aspects of relationships (e.g., one head might focus on syntactic relationships while another focuses on semantic ones), enriching the model's ability to capture complex dependencies.\n",
    "\n",
    "*   **Scaled Dot-Product Attention Formula:**\n",
    "    The formula for Scaled Dot-Product Attention is:\n",
    "    $$\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "    $$\n",
    "    *   `Q` (**Query**): A matrix representing a set of queries. In self-attention, this is a projection of the input sequence. Each query vector represents a word asking for attention from all other words.\n",
    "    *   `K` (**Key**): A matrix representing a set of keys. This is another projection of the input sequence. Each key vector can be thought of as a \"label\" for a word, which is matched against the queries.\n",
    "    *   `V` (**Value**): A matrix representing a set of values. This is a third projection of the input sequence. Each value vector contains the actual information of a word that should be passed on.\n",
    "    *   `d_k`: The dimension of the key vectors (and query vectors). The scaling factor `sqrt(d_k)` is crucial. For large values of `d_k`, the dot products can grow very large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Scaling counteracts this effect, leading to more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(114514)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax for a matrix.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q (np.array): Queries, shape (..., seq_len_q, d_k)\n",
    "        K (np.array): Keys, shape (..., seq_len_k, d_k)\n",
    "        V (np.array): Values, shape (..., seq_len_v, d_v)\n",
    "        mask (np.array, optional): Mask to apply. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output (np.array): Weighted sum of values.\n",
    "        attention_weights (np.array): Attention weights.\n",
    "    \"\"\"\n",
    "    # MatMul Q and K.T\n",
    "    matmul_qk = np.matmul(Q, K.swapaxes(-2, -1))\n",
    "    \n",
    "    # Scale\n",
    "    d_k = K.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = softmax(scaled_attention_logits)\n",
    "    \n",
    "    # MatMul with V\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "def multi_head_attention(embed_size, num_heads, input_seq, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate multi-head attention.\n",
    "    \n",
    "    Args:\n",
    "        embed_size (int): The embedding dimension.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        input_seq (np.array): The input sequence, shape (batch_size, seq_len, embed_size).\n",
    "        mask (np.array, optional): Mask to apply. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output (np.array): Final attention vector.\n",
    "        weights (np.array): Attention weights from one head for validation.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = input_seq.shape\n",
    "    assert embed_size % num_heads == 0, \"embed_size must be divisible by num_heads\"\n",
    "    \n",
    "    head_dim = embed_size // num_heads\n",
    "    \n",
    "    # Initialize weight matrices\n",
    "    Wq = np.random.randn(embed_size, embed_size)\n",
    "    Wk = np.random.randn(embed_size, embed_size)\n",
    "    Wv = np.random.randn(embed_size, embed_size)\n",
    "    Wo = np.random.randn(embed_size, embed_size)\n",
    "    \n",
    "    # 1. Linear projections\n",
    "    Q = np.matmul(input_seq, Wq)\n",
    "    K = np.matmul(input_seq, Wk)\n",
    "    V = np.matmul(input_seq, Wv)\n",
    "    \n",
    "    # 2. Reshape and transpose for multi-head\n",
    "    # (batch_size, seq_len, embed_size) -> (batch_size, seq_len, num_heads, head_dim) -> (batch_size, num_heads, seq_len, head_dim)\n",
    "    Q = Q.reshape(batch_size, seq_len, num_heads, head_dim).swapaxes(1, 2)\n",
    "    K = K.reshape(batch_size, seq_len, num_heads, head_dim).swapaxes(1, 2)\n",
    "    V = V.reshape(batch_size, seq_len, num_heads, head_dim).swapaxes(1, 2)\n",
    "    \n",
    "    # 3. Apply scaled dot-product attention\n",
    "    attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "    \n",
    "    # 4. Concatenate heads\n",
    "    # (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, num_heads, head_dim) -> (batch_size, seq_len, embed_size)\n",
    "    attention_output = attention_output.swapaxes(1, 2).reshape(batch_size, seq_len, embed_size)\n",
    "    \n",
    "    # 5. Final linear projection\n",
    "    output = np.matmul(attention_output, Wo)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 10\n",
    "    seq_len = 20\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    \n",
    "    input_data = np.random.randn(batch_size, seq_len, embed_size) \n",
    "    output, weights = multi_head_attention(embed_size, num_heads, input_data)\n",
    "    \n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Attention weights shape:\", weights.shape)\n",
    "    \n",
    "    print(\"\\nSample output vector (first 10 values of [0, 0]):\\n\", output[0, 0, :10])\n",
    "    # The weights shape is (batch_size, num_heads, seq_len, seq_len)\n",
    "    # We print the first head's attention for the first word in the first batch item\n",
    "    print(\"\\nSample attention weights (first 10 values of [0, 0, 0]):\\n\", weights[0, 0, 0, :10])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## 3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Based on PyTorch's multi-head self-attention implementation\"\"\"\n",
    "    def __init__(self, embed_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_size % num_heads == 0, \"embed_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V projections\n",
    "        self.wq = nn.Linear(embed_size, embed_size)\n",
    "        self.wk = nn.Linear(embed_size, embed_size)\n",
    "        self.wv = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        # Final output linear layer\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Q, K, V shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        d_k = K.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Apply linear layers\n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # (batch_size, seq_len, embed_size) -> (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        # (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, embed_size)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.fc_out(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test code\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(114514)\n",
    "    \n",
    "    # Construct test input (same shape as task 2)\n",
    "    batch_size = 10\n",
    "    seq_len = 20\n",
    "    embed_size = 128\n",
    "    num_heads = 8\n",
    "    \n",
    "    input_tensor = torch.randn(batch_size, seq_len, embed_size)\n",
    "    model = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "    # Perform self-attention (query=key=value)\n",
    "    output, attn_weights = model(input_tensor, input_tensor, input_tensor)\n",
    "\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Attention weights shape:\", attn_weights.shape)\n",
    "    \n",
    "    # Detach from graph for printing\n",
    "    output_detached = output.detach()\n",
    "    attn_weights_detached = attn_weights.detach()\n",
    "    \n",
    "    print(\"\\nSample output vector (first 10 values of [0, 0]):\\n\", output_detached[0, 0, :10])\n",
    "    print(\"\\nSample attention weights (first 10 values of [0, 0, 0]):\\n\", attn_weights_detached[0, 0, 0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca2aa1",
   "metadata": {},
   "source": [
    "### Subtask 4: Attention in Vision\n",
    "\n",
    "#### Image Serialization in ViT\n",
    "\n",
    "The Vision Transformer (ViT) converts a 2D image into a 1D sequence of vectors through a simple yet effective process:\n",
    "1.  **Image Patching**: The input image (e.g., 224x224 pixels) is first split into a grid of smaller, non-overlapping square patches (e.g., 16x16 pixels). This results in a sequence of `(224/16) * (224/16) = 14 * 14 = 196` patches.\n",
    "2.  **Flatten and Project**: Each of these 2D patches is then flattened into a 1D vector (e.g., `16 * 16 * 3` channels = 768 dimensions). This raw vector is then passed through a trainable linear projection (a simple fully connected layer) to produce a patch embedding of the desired model dimension (e.g., 768).\n",
    "This procedure effectively transforms the 2D image into a 1D sequence of token embeddings, analogous to a sequence of word embeddings in NLP, making it suitable for a standard Transformer encoder.\n",
    "\n",
    "#### Encoding Spatial Position Information\n",
    "\n",
    "Standard Transformers are permutation-invariant, meaning they treat the input as an unordered set. This is problematic for images where the spatial arrangement of patches is critical. ViT addresses this by explicitly adding spatial information:\n",
    "*   **Learnable Positional Embeddings**: Before the patch embeddings are fed into the Transformer encoder, a set of **learnable 1D positional embeddings** are added to them. There is one unique positional embedding vector for each possible patch position in the sequence. These positional embeddings are initialized randomly and are learned jointly with the rest of the model during training. By adding these vectors, the model can learn to interpret the relative and absolute positions of the patches in the original image.\n",
    "\n",
    "#### Core Advantage Over CNNs\n",
    "\n",
    "The most significant advantage of ViT's self-attention mechanism over a traditional CNN is its ability to model **long-range, global dependencies** from the very first layer.\n",
    "*   **CNNs (Local Receptive Fields)**: A convolutional kernel operates on a small, local neighborhood of pixels. To capture global context, a CNN must stack many layers, gradually increasing the receptive field size. Information from distant parts of the image can only be integrated in the deeper layers of the network.\n",
    "*   **ViT (Global Receptive Fields)**: In self-attention, every patch embedding (query) directly interacts with every other patch embedding (keys/values) in the sequence. This means that from the very first layer, the model can weigh and integrate information from across the entire image to compute the representation for a single patch. This global receptive field allows ViT to capture relationships between distant features more effectively, which is particularly powerful for tasks requiring an understanding of the overall image structure, especially when trained on very large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
