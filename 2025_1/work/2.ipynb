{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6142b41",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## 1. Env set up\n",
    "1. ... (already done before)\n",
    "2. pip install scikit-learn(including sklearn, scipy) torch\n",
    "---\n",
    "## 2. Learning/Working route\n",
    "1. figure out task-involved knowledge range, in this case package calling\n",
    "2. watch some short videos about these package to quickly get knowing what they do, have, and their advantages\n",
    "3. check their usage in the official documents\n",
    "4. learn new concepts like OneHot encoding, ask AI about its common realizations, and learn related methods\n",
    "5. ask AI about common ways of processing numerical data and do it under guidance\n",
    "6. when visualizing, using AI to help adjust the parameters\n",
    "---\n",
    "## 3. Work Sequentially\n",
    "#### a. Before start\n",
    "- numpy can read only numerical data, still use pandas to read\n",
    "- preprocess before further action:\n",
    "    1. filling missing data(with mean)\n",
    "    2. encode categorical features and 'international' col(using LabelEncoder)\n",
    "    3. prepare target var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cc652d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (6095, 8)\n",
      "Test set shape: (99, 8)\n",
      "Classes: ['Admit' 'Reject' 'Waitlist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxy84\\AppData\\Local\\Temp\\ipykernel_30948\\738569145.py:17: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col]=data[col].fillna(data[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_csv('../MBAAdmission/train.csv')\n",
    "test_data = pd.read_csv('../MBAAdmission/test.csv')\n",
    "\n",
    "# preprocess data\n",
    "def preprocess(data):\n",
    "    # handle missing values\n",
    "    num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data[num_cols] = data[num_cols].fillna(data[num_cols].mean())\n",
    "    for col in data.select_dtypes(include=[object]).columns:\n",
    "        if col != 'admission':\n",
    "            data[col]=data[col].fillna(data[col].mode()[0])\n",
    "    return\n",
    "\n",
    "preprocess(train_data)\n",
    "preprocess(test_data)\n",
    "\n",
    "categorical_cols = train_data.select_dtypes(include=[object]).columns\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# prepare features and labels\n",
    "\n",
    "# encode categorical variables\n",
    "X_train = train_data.drop(columns=['application_id', 'admission'])\n",
    "X_test = test_data.drop(columns=['application_id', 'admission'])\n",
    "\n",
    "# simple label encoding for categorical variables\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        le = sk.preprocessing.LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        # handle unseen labels in test set\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str)\n",
    "            X_test[col] = X_test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "            \n",
    "# encode 'international' column\n",
    "X_train['international'] = X_train['international'].astype(int)\n",
    "X_test['international'] = X_test['international'].astype(int)\n",
    "\n",
    "# target variable\n",
    "y_encoder = sk.preprocessing.LabelEncoder()\n",
    "y_train = y_encoder.fit_transform(train_data['admission'])\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Classes: {y_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d22a50",
   "metadata": {},
   "source": [
    "#### b. Subtask 1\n",
    "- call sk..LinearRegression, fit, predict and assess.\n",
    "- call LogisticReg, got warned that 1000 iter woundn't lead to convergence,\n",
    "try 3000, the same; 4000, converged.\n",
    "- output accuracy on both sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "deef1f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Linear Regression ---\n",
      "   Training Accuracy: 0.8466\n",
      "   Test Accuracy: 0.3333\n",
      "   Linear regression coefficients shape: (8,)\n",
      "\n",
      "--- Logistic Regression ---\n",
      "   Training Accuracy: 0.8409\n",
      "   Test Accuracy: 0.3838\n",
      "   Logistic regression coefficients shape: (3, 8)\n",
      "\n",
      "Regression Done.\n"
     ]
    }
   ],
   "source": [
    "# Subtask 1: Linear and Logistic Regression\n",
    "# a. Linear Regression\n",
    "print(\"\\n--- Linear Regression ---\")\n",
    "linear_reg = sk.linear_model.LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_train_pred_linear = linear_reg.predict(X_train)\n",
    "y_test_pred_linear = linear_reg.predict(X_test)\n",
    "\n",
    "# convert to int for classification\n",
    "y_train_pred_linear_class = np.round(np.clip(y_train_pred_linear, 0, len(y_encoder.classes_)-1)).astype(int)\n",
    "y_test_pred_linear_class = np.round(np.clip(y_test_pred_linear, 0, len(y_encoder.classes_)-1)).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_linear = sk.metrics.accuracy_score(y_train, y_train_pred_linear_class)\n",
    "test_acc_linear = sk.metrics.accuracy_score(y_encoder.transform(test_data['admission']), y_test_pred_linear_class)\n",
    "\n",
    "print(f\"   Training Accuracy: {train_acc_linear:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_linear:.4f}\")\n",
    "print(f\"   Linear regression coefficients shape: {linear_reg.coef_.shape}\")\n",
    "\n",
    "\n",
    "# b. Logistic Regression\n",
    "print(\"\\n--- Logistic Regression ---\")\n",
    "logistic_reg = sk.linear_model.LogisticRegression(max_iter=4000, random_state=42)\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_train_pred_logistic = logistic_reg.predict(X_train)\n",
    "y_test_pred_logistic = logistic_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_logistic = sk.metrics.accuracy_score(y_train, y_train_pred_logistic)\n",
    "test_acc_logistic = sk.metrics.accuracy_score(y_encoder.transform(test_data['admission']), y_test_pred_logistic)\n",
    "\n",
    "print(f\"   Training Accuracy: {train_acc_logistic:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_logistic:.4f}\")\n",
    "print(f\"   Logistic regression coefficients shape: {logistic_reg.coef_.shape}\")\n",
    "\n",
    "print(\"\\nRegression Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac915a",
   "metadata": {},
   "source": [
    "#### c. Subtask 2\n",
    "- normalize for better training\n",
    "- build model, thanks AI for helping fill in the parameters\n",
    "- Glad to see 'adam' again btw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6083c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MLP Classifier ---\n",
      "Scaled Training set shape: (6095, 8)\n",
      "Scaled Test set shape: (99, 8)\n",
      "Network Architecture: [8, 128] -> [128, 256] -> [256, 3]\n",
      "Training MLP...\n",
      "Iteration 1, loss = 0.67510977\n",
      "Iteration 2, loss = 0.43760726\n",
      "Iteration 3, loss = 0.40227626\n",
      "Iteration 4, loss = 0.39255596\n",
      "Iteration 5, loss = 0.38850201\n",
      "Iteration 6, loss = 0.38602716\n",
      "Iteration 7, loss = 0.38351205\n",
      "Iteration 8, loss = 0.38231942\n",
      "Iteration 9, loss = 0.38056649\n",
      "Iteration 10, loss = 0.37944881\n",
      "Iteration 11, loss = 0.37815923\n",
      "Iteration 12, loss = 0.37692867\n",
      "Iteration 13, loss = 0.37576888\n",
      "Iteration 14, loss = 0.37473733\n",
      "Iteration 15, loss = 0.37371770\n",
      "Iteration 16, loss = 0.37291974\n",
      "Iteration 17, loss = 0.37199046\n",
      "Iteration 18, loss = 0.37133703\n",
      "Iteration 19, loss = 0.37025144\n",
      "Iteration 20, loss = 0.36922436\n",
      "Iteration 21, loss = 0.36881741\n",
      "Iteration 22, loss = 0.36800906\n",
      "Iteration 23, loss = 0.36734866\n",
      "Iteration 24, loss = 0.36671220\n",
      "Iteration 25, loss = 0.36567163\n",
      "Iteration 26, loss = 0.36491075\n",
      "Iteration 27, loss = 0.36455577\n",
      "Iteration 28, loss = 0.36396612\n",
      "Iteration 29, loss = 0.36276463\n",
      "Iteration 30, loss = 0.36239832\n",
      "Iteration 31, loss = 0.36154137\n",
      "Iteration 32, loss = 0.36070549\n",
      "Iteration 33, loss = 0.36022557\n",
      "Iteration 34, loss = 0.35966707\n",
      "Iteration 35, loss = 0.35887080\n",
      "Iteration 36, loss = 0.35845901\n",
      "Iteration 37, loss = 0.35774501\n",
      "Iteration 38, loss = 0.35696175\n",
      "Iteration 39, loss = 0.35650907\n",
      "Iteration 40, loss = 0.35605412\n",
      "Iteration 41, loss = 0.35582369\n",
      "Iteration 42, loss = 0.35492258\n",
      "Iteration 43, loss = 0.35428124\n",
      "Iteration 44, loss = 0.35377763\n",
      "Iteration 45, loss = 0.35332961\n",
      "Iteration 46, loss = 0.35256946\n",
      "Iteration 47, loss = 0.35227070\n",
      "Iteration 48, loss = 0.35147550\n",
      "Iteration 49, loss = 0.35129785\n",
      "Iteration 50, loss = 0.35041602\n",
      "MLP Training Complete.\n",
      "   Training Accuracy: 0.8527\n",
      "   Test Accuracy: 0.3535\n",
      "   Total parameters: 34947\n",
      "MLP Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\scoop\\apps\\python\\3.13.7\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Subtask 2: sklearn MLP Classifier\n",
    "print(\"\\n--- MLP Classifier ---\")\n",
    "\n",
    "# normalize features\n",
    "scaler = sk.preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled Test set shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# build and train MLP using provided parameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "output_dim = len(y_encoder.classes_)\n",
    "\n",
    "print(f\"Network Architecture: [{input_dim}, 128] -> [128, 256] -> [256, {output_dim}]\")\n",
    "\n",
    "# build model\n",
    "mlp = sk.neural_network.MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 256),  # Two hidden layers: 128 and 256 neurons\n",
    "    activation='relu',              # ReLU activation\n",
    "    learning_rate_init=0.0001,      # lr = 0.0001\n",
    "    max_iter=50,                    # epoch = 50\n",
    "    batch_size=64,                  # batch = 64\n",
    "    random_state=42,\n",
    "    solver='adam',                  # Adam optimizer (sklearn default)\n",
    "    early_stopping=False,           # Don't stop early to complete all epochs\n",
    "    verbose=True                    # Show training progress\n",
    ")\n",
    "\n",
    "# train model\n",
    "print(\"Training MLP...\")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "print(\"MLP Training Complete.\")\n",
    "\n",
    "# make predictions\n",
    "y_train_pred_mlp = mlp.predict(X_train_scaled)\n",
    "y_test_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_mlp = sk.metrics.accuracy_score(y_train, y_train_pred_mlp)\n",
    "test_acc_mlp = sk.metrics.accuracy_score(y_encoder.transform(test_data['admission']), y_test_pred_mlp)\n",
    "print(f\"   Training Accuracy: {train_acc_mlp:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_mlp:.4f}\")\n",
    "total_params = sum(coef.size for coef in mlp.coefs_) + sum(bias.size for bias in mlp.intercepts_)\n",
    "print(f\"   Total parameters: {total_params}\")\n",
    "print(\"MLP Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd277752",
   "metadata": {},
   "source": [
    "#### d. Subtask 3\n",
    "- install torch\n",
    "- search and see what 'super' class is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee526a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask 3: Torch MLP Classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# define MLP model\n",
    "class MLP_PyTorch(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLP_PyTorch, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
