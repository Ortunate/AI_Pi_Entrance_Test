{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6142b41",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## 1. Env set up\n",
    "1. ... (already done before)\n",
    "2. pip install scikit-learn(including sklearn, scipy) torch\n",
    "---\n",
    "## 2. Learning/Working route\n",
    "1. figure out task-involved knowledge range, in this case package calling\n",
    "2. watch some short videos about these package to quickly get knowing what they do, have, and their advantages\n",
    "3. check their usage in the official documents\n",
    "4. learn new concepts like OneHot encoding, ask AI about its common realizations, and learn related methods\n",
    "5. ask AI about common ways of processing numerical data and do it under guidance\n",
    "6. when visualizing, using AI to help adjust the parameters\n",
    "---\n",
    "## 3. Work Sequentially\n",
    "#### a. Before start\n",
    "- numpy can read only numerical data, still use pandas to read\n",
    "- preprocess before further action:\n",
    "    1. filling missing data(with mean)\n",
    "    2. encode categorical features and 'international' col(using LabelEncoder)\n",
    "    3. prepare target var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc652d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (6095, 8)\n",
      "Test set shape: (99, 8)\n",
      "Classes: ['Admit' 'Reject' 'Waitlist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxy84\\AppData\\Local\\Temp\\ipykernel_2908\\2139849652.py:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[col]=data[col].fillna(data[col].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "lr = 0.0001\n",
    "epoch = 50\n",
    "batch = 64\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_csv('../MBAAdmission/train.csv')\n",
    "test_data = pd.read_csv('../MBAAdmission/test.csv')\n",
    "\n",
    "# preprocess data\n",
    "def preprocess(data):\n",
    "    # handle missing values\n",
    "    num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data[num_cols] = data[num_cols].fillna(data[num_cols].mean())\n",
    "    for col in data.select_dtypes(include=[object]).columns:\n",
    "        if col != 'admission':\n",
    "            data[col]=data[col].fillna(data[col].mode()[0])\n",
    "    return\n",
    "\n",
    "preprocess(train_data)\n",
    "preprocess(test_data)\n",
    "\n",
    "categorical_cols = train_data.select_dtypes(include=[object]).columns\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# prepare features and labels\n",
    "\n",
    "# encode categorical variables\n",
    "X_train = train_data.drop(columns=['application_id', 'admission'])\n",
    "X_test = test_data.drop(columns=['application_id', 'admission'])\n",
    "\n",
    "# simple label encoding for categorical variables\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        le = sk.preprocessing.LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        # handle unseen labels in test set\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str)\n",
    "            X_test[col] = X_test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
    "            \n",
    "# encode 'international' column\n",
    "X_train['international'] = X_train['international'].astype(int)\n",
    "X_test['international'] = X_test['international'].astype(int)\n",
    "\n",
    "# target variable\n",
    "y_encoder = sk.preprocessing.LabelEncoder()\n",
    "y_train = y_encoder.fit_transform(train_data['admission'])\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Classes: {y_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d22a50",
   "metadata": {},
   "source": [
    "#### b. Subtask 1\n",
    "- call sk..LinearRegression, fit, predict and assess.\n",
    "- call LogisticReg, got warned that 1000 iter woundn't lead to convergence,\n",
    "try 3000, the same; 4000, converged.\n",
    "- output accuracy on both sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deef1f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Linear Regression ---\n",
      "   Training Accuracy: 0.8466\n",
      "   Test Accuracy: 0.3333\n",
      "   Linear regression coefficients shape: (8,)\n",
      "\n",
      "--- Logistic Regression ---\n",
      "   Training Accuracy: 0.8409\n",
      "   Test Accuracy: 0.3838\n",
      "   Logistic regression coefficients shape: (3, 8)\n",
      "\n",
      "Regression Done.\n"
     ]
    }
   ],
   "source": [
    "# Subtask 1: Linear and Logistic Regression\n",
    "# a. Linear Regression\n",
    "print(\"\\n--- Linear Regression ---\")\n",
    "linear_reg = sk.linear_model.LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_train_pred_linear = linear_reg.predict(X_train)\n",
    "y_test_pred_linear = linear_reg.predict(X_test)\n",
    "\n",
    "# convert to int for classification\n",
    "y_train_pred_linear_class = np.round(np.clip(y_train_pred_linear, 0, len(y_encoder.classes_)-1)).astype(int)\n",
    "y_test_pred_linear_class = np.round(np.clip(y_test_pred_linear, 0, len(y_encoder.classes_)-1)).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_linear = sk.metrics.accuracy_score(y_train, y_train_pred_linear_class)\n",
    "test_acc_linear = sk.metrics.accuracy_score(y_encoder.transform(test_data['admission']), y_test_pred_linear_class)\n",
    "\n",
    "print(f\"   Training Accuracy: {train_acc_linear:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_linear:.4f}\")\n",
    "print(f\"   Linear regression coefficients shape: {linear_reg.coef_.shape}\")\n",
    "\n",
    "\n",
    "# b. Logistic Regression\n",
    "print(\"\\n--- Logistic Regression ---\")\n",
    "logistic_reg = sk.linear_model.LogisticRegression(max_iter=4000, random_state=42)\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_train_pred_logistic = logistic_reg.predict(X_train)\n",
    "y_test_pred_logistic = logistic_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_logistic = sk.metrics.accuracy_score(y_train, y_train_pred_logistic)\n",
    "test_acc_logistic = sk.metrics.accuracy_score(y_encoder.transform(test_data['admission']), y_test_pred_logistic)\n",
    "\n",
    "print(f\"   Training Accuracy: {train_acc_logistic:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_logistic:.4f}\")\n",
    "print(f\"   Logistic regression coefficients shape: {logistic_reg.coef_.shape}\")\n",
    "\n",
    "print(\"\\nRegression Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac915a",
   "metadata": {},
   "source": [
    "#### c. Subtask 2\n",
    "- learn the common process using sklearn to train a MLP model\n",
    "- normalize for better training\n",
    "- build model\n",
    "- random_state and '42': a widely used meme seed LOL\n",
    "- Glad to see 'adam' again btw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6083c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MLP Classifier ---\n",
      "Scaled Training set shape: (6095, 8)\n",
      "Scaled Test set shape: (99, 8)\n",
      "Network Architecture: [8, 128] -> [128, 256] -> [256, 3]\n",
      "Training MLP...\n",
      "Iteration 1, loss = 0.67510977\n",
      "Iteration 2, loss = 0.43760726\n",
      "Iteration 3, loss = 0.40227626\n",
      "Iteration 4, loss = 0.39255596\n",
      "Iteration 5, loss = 0.38850201\n",
      "Iteration 6, loss = 0.38602716\n",
      "Iteration 7, loss = 0.38351205\n",
      "Iteration 8, loss = 0.38231942\n",
      "Iteration 9, loss = 0.38056649\n",
      "Iteration 10, loss = 0.37944881\n",
      "Iteration 11, loss = 0.37815923\n",
      "Iteration 12, loss = 0.37692867\n",
      "Iteration 13, loss = 0.37576888\n",
      "Iteration 14, loss = 0.37473733\n",
      "Iteration 15, loss = 0.37371770\n",
      "Iteration 16, loss = 0.37291974\n",
      "Iteration 17, loss = 0.37199046\n",
      "Iteration 18, loss = 0.37133703\n",
      "Iteration 19, loss = 0.37025144\n",
      "Iteration 20, loss = 0.36922436\n",
      "Iteration 21, loss = 0.36881741\n",
      "Iteration 22, loss = 0.36800906\n",
      "Iteration 23, loss = 0.36734866\n",
      "Iteration 24, loss = 0.36671220\n",
      "Iteration 25, loss = 0.36567163\n",
      "Iteration 26, loss = 0.36491075\n",
      "Iteration 27, loss = 0.36455577\n",
      "Iteration 28, loss = 0.36396612\n",
      "Iteration 29, loss = 0.36276463\n",
      "Iteration 30, loss = 0.36239832\n",
      "Iteration 31, loss = 0.36154137\n",
      "Iteration 32, loss = 0.36070549\n",
      "Iteration 33, loss = 0.36022557\n",
      "Iteration 34, loss = 0.35966707\n",
      "Iteration 35, loss = 0.35887080\n",
      "Iteration 36, loss = 0.35845901\n",
      "Iteration 37, loss = 0.35774501\n",
      "Iteration 38, loss = 0.35696175\n",
      "Iteration 39, loss = 0.35650907\n",
      "Iteration 40, loss = 0.35605412\n",
      "Iteration 41, loss = 0.35582369\n",
      "Iteration 42, loss = 0.35492258\n",
      "Iteration 43, loss = 0.35428124\n",
      "Iteration 44, loss = 0.35377763\n",
      "Iteration 45, loss = 0.35332961\n",
      "Iteration 46, loss = 0.35256946\n",
      "Iteration 47, loss = 0.35227070\n",
      "Iteration 48, loss = 0.35147550\n",
      "Iteration 49, loss = 0.35129785\n",
      "Iteration 50, loss = 0.35041602\n",
      "MLP Training Complete.\n",
      "   Training Accuracy: 0.8527\n",
      "   Test Accuracy: 0.3535\n",
      "   Total parameters: 34947\n",
      "MLP Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\scoop\\apps\\python\\3.13.7\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Subtask 2: sklearn MLP Classifier\n",
    "print(\"\\n--- MLP Classifier ---\")\n",
    "\n",
    "# normalize features\n",
    "scaler = sk.preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled Test set shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# build and train MLP using provided parameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "output_dim = len(y_encoder.classes_)\n",
    "\n",
    "print(f\"Network Architecture: [{input_dim}, 128] -> [128, 256] -> [256, {output_dim}]\")\n",
    "\n",
    "# build model\n",
    "mlp = sk.neural_network.MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 256),  # Two hidden layers: 128 and 256 neurons\n",
    "    activation='relu',              # ReLU activation\n",
    "    learning_rate_init=lr,      # lr = 0.0001\n",
    "    max_iter=epoch,                    # epoch = 50\n",
    "    batch_size=batch,                  # batch = 64\n",
    "    random_state=42,\n",
    "    solver='adam',                  # Adam optimizer (sklearn default)\n",
    "    early_stopping=False,           # Don't stop early to complete all epochs\n",
    "    verbose=True                    # Show training progress\n",
    ")\n",
    "\n",
    "# train model\n",
    "print(\"Training MLP...\")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "print(\"MLP Training Complete.\")\n",
    "\n",
    "# make predictions\n",
    "y_train_pred_mlp = mlp.predict(X_train_scaled)\n",
    "y_test_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_mlp = sk.metrics.accuracy_score(y_train, y_train_pred_mlp)\n",
    "test_acc_mlp = sk.metrics.accuracy_score(y_encoder.transform(test_data['admission']), y_test_pred_mlp)\n",
    "print(f\"   Training Accuracy: {train_acc_mlp:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_mlp:.4f}\")\n",
    "total_params = sum(coef.size for coef in mlp.coefs_) + sum(bias.size for bias in mlp.intercepts_)\n",
    "print(f\"   Total parameters: {total_params}\")\n",
    "print(\"MLP Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd277752",
   "metadata": {},
   "source": [
    "#### d. Subtask 3\n",
    "- install torch\n",
    "- search and see what 'super' class is: a func allows calling methods from parent class\n",
    "- why commonly a class is built using Torch but not sklearn: PyTorch is more customizable while scikit-learn provides standard ML.\n",
    "- adopt the same optimizer 'Adam' as above\n",
    "- learn the training process using Torch\n",
    "- learn some apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee526a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training PyTorch MLP...\n",
      "Epoch 1/0, Loss: 0.7482\n",
      "Epoch 2/1, Loss: 0.4593\n",
      "Epoch 3/2, Loss: 0.4058\n",
      "Epoch 4/3, Loss: 0.3910\n",
      "Epoch 5/4, Loss: 0.3868\n",
      "Epoch 6/5, Loss: 0.3845\n",
      "Epoch 7/6, Loss: 0.3831\n",
      "Epoch 8/7, Loss: 0.3815\n",
      "Epoch 9/8, Loss: 0.3806\n",
      "Epoch 10/9, Loss: 0.3790\n",
      "Epoch 11/10, Loss: 0.3781\n",
      "Epoch 12/11, Loss: 0.3771\n",
      "Epoch 13/12, Loss: 0.3763\n",
      "Epoch 14/13, Loss: 0.3755\n",
      "Epoch 15/14, Loss: 0.3746\n",
      "Epoch 16/15, Loss: 0.3739\n",
      "Epoch 17/16, Loss: 0.3731\n",
      "Epoch 18/17, Loss: 0.3722\n",
      "Epoch 19/18, Loss: 0.3715\n",
      "Epoch 20/19, Loss: 0.3706\n",
      "Epoch 21/20, Loss: 0.3696\n",
      "Epoch 22/21, Loss: 0.3693\n",
      "Epoch 23/22, Loss: 0.3683\n",
      "Epoch 24/23, Loss: 0.3675\n",
      "Epoch 25/24, Loss: 0.3670\n",
      "Epoch 26/25, Loss: 0.3661\n",
      "Epoch 27/26, Loss: 0.3654\n",
      "Epoch 28/27, Loss: 0.3642\n",
      "Epoch 29/28, Loss: 0.3648\n",
      "Epoch 30/29, Loss: 0.3636\n",
      "Epoch 31/30, Loss: 0.3623\n",
      "Epoch 32/31, Loss: 0.3621\n",
      "Epoch 33/32, Loss: 0.3612\n",
      "Epoch 34/33, Loss: 0.3602\n",
      "Epoch 35/34, Loss: 0.3600\n",
      "Epoch 36/35, Loss: 0.3592\n",
      "Epoch 37/36, Loss: 0.3582\n",
      "Epoch 38/37, Loss: 0.3578\n",
      "Epoch 39/38, Loss: 0.3572\n",
      "Epoch 40/39, Loss: 0.3562\n",
      "Epoch 41/40, Loss: 0.3557\n",
      "Epoch 42/41, Loss: 0.3558\n",
      "Epoch 43/42, Loss: 0.3543\n",
      "Epoch 44/43, Loss: 0.3536\n",
      "Epoch 45/44, Loss: 0.3532\n",
      "Epoch 46/45, Loss: 0.3521\n",
      "Epoch 47/46, Loss: 0.3517\n",
      "Epoch 48/47, Loss: 0.3511\n",
      "Epoch 49/48, Loss: 0.3508\n",
      "Epoch 50/49, Loss: 0.3496\n",
      "PyTorch MLP Training Complete.\n",
      "   Training Accuracy: 0.8525\n",
      "   Test Accuracy: 0.3535\n",
      "   Total parameters: 34947\n",
      "PyTorch MLP Done.\n",
      "\n",
      "--- Comparison ---\n",
      "Sklearn MLP - Training Accuracy: 0.8527, Test Accuracy: 0.3535, Total Params: 34947\n",
      "PyTorch MLP - Training Accuracy: 0.8525, Test Accuracy: 0.3535, Total Params: 34947\n",
      "Optimization Rate - Torch over Sklearn\n",
      "- Training: -0.02%, Test: 0.00%\n",
      "Comparison Done.\n"
     ]
    }
   ],
   "source": [
    "# Subtask 3: Torch MLP Classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# define MLP model\n",
    "class MLP_PyTorch(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLP_PyTorch, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# prepare data for PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "# create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# initialize model, loss function, optimizer\n",
    "model = MLP_PyTorch(input_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# train model\n",
    "print(\"\\nTraining PyTorch MLP...\")\n",
    "model.train()\n",
    "for epoch in range(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epoch}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "print(\"PyTorch MLP Training Complete.\")\n",
    "\n",
    "# evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Training accuracy\n",
    "    train_outputs = model(X_train_tensor)\n",
    "    _, train_preds = torch.max(train_outputs, 1)\n",
    "    train_acc_torch = (train_preds == y_train_tensor).float().mean().item()\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, test_preds = torch.max(test_outputs, 1)\n",
    "    test_acc_torch = (test_preds == torch.LongTensor(y_encoder.transform(test_data['admission']))).float().mean().item()\n",
    "    \n",
    "print(f\"   Training Accuracy: {train_acc_torch:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_acc_torch:.4f}\")\n",
    "total_params_torch = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params_torch}\")\n",
    "\n",
    "print(\"PyTorch MLP Done.\")\n",
    "\n",
    "\n",
    "# compare sklearn and PyTorch\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(f\"Sklearn MLP - Training Accuracy: {train_acc_mlp:.4f}, Test Accuracy: {test_acc_mlp:.4f}, Total Params: {total_params}\")\n",
    "print(f\"PyTorch MLP - Training Accuracy: {train_acc_torch:.4f}, Test Accuracy: {test_acc_torch:.4f}, Total Params: {total_params_torch}\")\n",
    "# optimization rate\n",
    "opt_rate_train = (train_acc_torch - train_acc_mlp) / train_acc_mlp * 100\n",
    "opt_rate_test = (test_acc_torch - test_acc_mlp) / test_acc_mlp * 100\n",
    "print(f\"Optimization Rate - Torch over Sklearn\\n- Training: {opt_rate_train:.2f}%, Test: {opt_rate_test:.2f}%\")\n",
    "print(\"Comparison Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d0ab9",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "##### 1. diff between sklearn and PyTorch may come from\n",
    "1. Different default initialization\n",
    "2. Different optimization algorithms implementations(sklearn vs PyTorch Adam)\n",
    "3. Different batch handling, shuffling, etc.\n",
    "4. Different numerical precision\n",
    "5. Different regularization defaults\n",
    "##### 2. diff between train/test accuracy likely due to\n",
    "1. Overfitting - model fits training data too well\n",
    "2. Limited training data, insufficient sample\n",
    "3. Model complexity too high for dataset size\n",
    "4. Distribution shift between train and test sets\n",
    "5. too few epoches, the model may not converge\n",
    "\n",
    "#### e. Subtask 4\n",
    "- consult names for the activation funcs in package\n",
    "- train and run model with each\n",
    "- output comparison and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66844993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Activation Function Comparison ---\n",
      "Testing activation function: identity\n",
      "   Training Accuracy: 0.8384\n",
      "   Final Loss: 0.391846\n",
      "   Iterations: 18\n",
      "\n",
      "Testing activation function: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\scoop\\apps\\python\\3.13.7\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training Accuracy: 0.8525\n",
      "   Final Loss: 0.351298\n",
      "   Iterations: 49\n",
      "\n",
      "Testing activation function: logistic\n",
      "   Training Accuracy: 0.8405\n",
      "   Final Loss: 0.389910\n",
      "   Iterations: 49\n",
      "\n",
      "Activation Function Comparison:\n",
      "Function     Accuracy   Loss       Iterations  \n",
      "--------------------------------------------------\n",
      "identity     0.8384     0.391846   18          \n",
      "relu         0.8525     0.351298   49          \n",
      "logistic     0.8405     0.389910   49          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\scoop\\apps\\python\\3.13.7\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Subtask 4: Activation Function Comparison\n",
    "print(\"\\n--- Activation Function Comparison ---\")\n",
    "\n",
    "# Test different activation functions using sklearn MLPClassifier\n",
    "# identity=no activation, logistic=sigmoid\n",
    "activation_functions = ['identity', 'relu', 'logistic']  \n",
    "results = {}\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print(f\"Testing activation function: {activation}\")\n",
    "    \n",
    "    mlp_activation = sk.neural_network.MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 256),\n",
    "        activation=activation,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=epoch,\n",
    "        batch_size=batch,\n",
    "        random_state=42,\n",
    "        solver='adam',\n",
    "        early_stopping=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    mlp_activation.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = sk.metrics.accuracy_score(y_train, mlp_activation.predict(X_train_scaled))\n",
    "    \n",
    "    results[activation] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'final_loss': mlp_activation.loss_,\n",
    "        'iterations': mlp_activation.n_iter_\n",
    "    }\n",
    "    \n",
    "    print(f\"   Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"   Final Loss: {mlp_activation.loss_:.6f}\")\n",
    "    print(f\"   Iterations: {mlp_activation.n_iter_}\\n\")\n",
    "\n",
    "print(\"Activation Function Comparison:\")\n",
    "print(f\"{'Function':<12} {'Accuracy':<10} {'Loss':<10} {'Iterations':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for func, metrics in results.items():\n",
    "    print(f\"{func:<12} {metrics['train_accuracy']:<10.4f} {metrics['final_loss']:<10.6f} {metrics['iterations']:<12}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4baf99",
   "metadata": {},
   "source": [
    "#### Activation Function Analysis:\n",
    "\n",
    "As I learned, an activation func in ML(especially NN):\n",
    "- introduces non-linearity which allows learning complex patterns and relationships\n",
    "\n",
    "For this dataset and model configuration:\n",
    "- 'No Activation' converges fastest in perceivable contrast.\n",
    "- 'relu' achieves best accuracy and loss.\n",
    "- 'logistic' makes slightly better result but cost more than 'No Activation'\n",
    "\n",
    "For each:\n",
    "1. No Activation (identity):\n",
    "   - f(x) = x\n",
    "   - Linear transformations only\n",
    "   - Cannot learn complex non-linear patterns\n",
    "   - Equivalent to linear regression for classification(no non-linearity)\n",
    "\n",
    "2. ReLU Activation:\n",
    "   - Introduces non-linearity: f(x) = max(0, x)\n",
    "   - Addresses vanishing gradient problem\n",
    "   - Most commonly used in deep networks\n",
    "   - Can suffer from 'dying ReLU' problem\n",
    "   - somehow like a neuron, yield a weight when active\n",
    "\n",
    "3. Sigmoid Activation:\n",
    "   - f(x) = 1/(1+exp(-x))\n",
    "   - Smooth non-linearity\n",
    "   - infinitely differentiable and infinitely smooth \n",
    "   - Output range [0,1]\n",
    "   - Can suffer from vanishing gradients in deep networks\n",
    "   - sensitive around x=0\n",
    "   - Saturates for large input values\n",
    "\n",
    "\n",
    "#### f. Subtask 5\n",
    "- try:\n",
    "   1. aa\n",
    "- and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cced5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask 5: Feature processing for better performance\n",
    "print(\"\\n--- Feature Processing for Better Performance ---\")\n",
    "\n",
    "# Baseline performance\n",
    "baseline_acc = train_acc_mlp\n",
    "print(f\"Baseline MLP accuracy: {baseline_acc:.4f}\")\n",
    "\n",
    "# Try 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fef882",
   "metadata": {},
   "source": [
    "## **Notations**\n",
    "\n",
    "- $\\mathbf{x}$: Input vector (shape: $[d_{in}, 1]$)\n",
    "- $\\mathbf{W}_1, \\mathbf{b}_1$: Weights and bias for first hidden layer ($[128, d_{in}]$, $[128, 1]$)\n",
    "- $\\mathbf{W}_2, \\mathbf{b}_2$: Weights and bias for second hidden layer ($[256, 128]$, $[256, 1]$)\n",
    "- $\\mathbf{W}_3, \\mathbf{b}_3$: Weights and bias for output layer ($[d_{out}, 256]$, $[d_{out}, 1]$)\n",
    "- $\\mathrm{ReLU}(z)$: Activation function, $\\mathrm{ReLU}(z) = \\max(0, z)$\n",
    "- $\\odot$: Element-wise (Hadamard) product\n",
    "- $L$: Loss function (e.g., cross-entropy)\n",
    "- $\\delta_i$: Gradient of loss w.r.t. pre-activation at layer $i$ (i.e., $\\delta_i = \\frac{\\partial L}{\\partial \\mathbf{z}_i}$)\n",
    "\n",
    "---\n",
    "\n",
    "## **Forward Pass**\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{a}_1 &= \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1 \\\\\n",
    "\\mathbf{h}_1 &= \\mathrm{ReLU}(\\mathbf{a}_1) \\\\\n",
    "\\mathbf{a}_2 &= \\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2 \\\\\n",
    "\\mathbf{h}_2 &= \\mathrm{ReLU}(\\mathbf{a}_2) \\\\\n",
    "\\mathbf{z}   &= \\mathbf{W}_3 \\mathbf{h}_2 + \\mathbf{b}_3 \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **Backward Pass (Backpropagation)**\n",
    "\n",
    "### 1. **Output Layer**\n",
    "\n",
    "\\[\n",
    "\\delta_3 = \\frac{\\partial L}{\\partial \\mathbf{z}}\n",
    "\\]\n",
    "\n",
    "- For cross-entropy with softmax, $\\delta_3 = \\hat{\\mathbf{y}} - \\mathbf{y}$\n",
    "\n",
    "### 2. **Second Hidden Layer**\n",
    "\n",
    "\\[\n",
    "\\delta_2 = (\\mathbf{W}_3^\\top \\delta_3) \\odot \\mathrm{ReLU}'(\\mathbf{a}_2)\n",
    "\\]\n",
    "- $\\mathrm{ReLU}'(\\mathbf{a}_2)$ is $1$ where $\\mathbf{a}_2 > 0$, else $0$\n",
    "\n",
    "### 3. **First Hidden Layer**\n",
    "\n",
    "\\[\n",
    "\\delta_1 = (\\mathbf{W}_2^\\top \\delta_2) \\odot \\mathrm{ReLU}'(\\mathbf{a}_1)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **Gradients w.r.t. Parameters**\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_3} &= \\delta_3 \\mathbf{h}_2^\\top \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}_3} &= \\delta_3 \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_2} &= \\delta_2 \\mathbf{h}_1^\\top \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}_2} &= \\delta_2 \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_1} &= \\delta_1 \\mathbf{x}^\\top \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}_1} &= \\delta_1 \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Steps**\n",
    "\n",
    "1. **Forward:** Compute activations $\\mathbf{a}_1, \\mathbf{h}_1, \\mathbf{a}_2, \\mathbf{h}_2, \\mathbf{z}$\n",
    "2. **Backward:**  \n",
    "   - Compute $\\delta_3$ from loss  \n",
    "   - Propagate to $\\delta_2$ using $\\mathbf{W}_3$ and ReLU derivative  \n",
    "   - Propagate to $\\delta_1$ using $\\mathbf{W}_2$ and ReLU derivative  \n",
    "3. **Parameter Gradients:** Use $\\delta_i$ and previous layer activations to get gradients for all weights and biases.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
