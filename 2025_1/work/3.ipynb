{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ff094e",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## 1. Env set up\n",
    "1. ... already set up\n",
    "---\n",
    "## 2. Work Sequentially\n",
    "- using Copilot(Gemini 2.5 pro) to help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0608f",
   "metadata": {},
   "source": [
    "#### a. Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643f945c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Net A:\n",
      "Output after first linear layer:\n",
      " [[-5 -3 -1  1  3]]\n",
      "Output after second linear layer:\n",
      " [[ 5.5  3.5  1.5 -0.5 -2.5]]\n",
      "\n",
      "Net B with Non-linearity:\n",
      "Output after first linear layer:\n",
      " [[-5 -3 -1  1  3]]\n",
      "Output after ReLU:\n",
      " [[0 0 0 1 3]]\n",
      "Output after second linear layer:\n",
      " [[ 0.5  0.5  0.5 -0.5 -2.5]]\n"
     ]
    }
   ],
   "source": [
    "# Subtask 1\n",
    "import numpy as np\n",
    "\n",
    "def linear_layer(x, w, b):\n",
    "    return np.dot(x, w) + b\n",
    "    \n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    对输入张量 x 执行元素级的 ReLU (Rectified Linear Unit) 操作。\n",
    "    公式为: f(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    # ===== 在此实现 =====\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def flatten(x):\n",
    "    \"\"\"\n",
    "    将一个四维张量 (N, C, H, W) 展平为一个二维张量 (N, C*H*W)。\n",
    "    N 是批量大小，需要保持不变。\n",
    "    \"\"\"\n",
    "    # ===== 在此实现 =====\n",
    "    N = x.shape[0]\n",
    "    return x.reshape(N, -1)\n",
    "\n",
    "\n",
    "# comparison experiment\n",
    "x = np.array([[-2], [-1], [0], [1], [2]])\n",
    "w1, b1 = np.array([[2]]), np.array([-1])\n",
    "w2, b2 = np.array([[-1]]), np.array([0.5])\n",
    "\n",
    "# linear net A\n",
    "print(\"Linear Net A:\")\n",
    "out_a1 = linear_layer(x, w1, b1)\n",
    "out_a2 = linear_layer(out_a1, w2, b2)\n",
    "print(\"Output after first linear layer:\\n\", out_a1.T)\n",
    "print(\"Output after second linear layer:\\n\", out_a2.T)\n",
    "\n",
    "# net B with Non-linearity\n",
    "print(\"\\nNet B with Non-linearity:\")\n",
    "out_b1 = linear_layer(x, w1, b1)\n",
    "out_b2 = relu(out_b1)\n",
    "out_b3 = linear_layer(out_b2, w2, b2)\n",
    "print(\"Output after first linear layer:\\n\", out_b1.T)\n",
    "print(\"Output after ReLU:\\n\", out_b2.T)\n",
    "print(\"Output after second linear layer:\\n\", out_b3.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b24dd",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "(1)\n",
    "1. relu in this case changed all negative values to 0 and left positive values unchanged.\n",
    "2. original input is linearly distributed, and since net A is pure linear transformation, the output is also linearly distributed; while net B with relu breaks this linearity, making the output non-linearly distributed.\n",
    "\n",
    "(2)\n",
    "- Non-linear activation funcs are essential because however complex a linear func combination can be, it is still linear. Yet we need the model to learn and handle complex non-linear real-world data, which means non-linearity must be introduced, and this is why non-linear activation funcs matter.\n",
    "\n",
    "(3)\n",
    "- The flatten layer is used to convert multi-dimensional data into one-dimensional data, which is much easier to process(as a tensor). This can help to connect different types of layers, like between input layers and dense layers, CNN layers and dense layers, etc. Also we commonly use generators to convert low-dimensional data to high-dimensional data, which is like the opposite of flattening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794d09b",
   "metadata": {},
   "source": [
    "#### b. Subtask 2\n",
    "- Thinking:\n",
    "1. A fully connected layer needs 100\\*100\\*1(weights)+1(total bias) = 10001 parameters.\n",
    "2. a 3\\*3 kernel needs 9\\*1(weights)+1(bias) = 10 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26190ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for centered cross pattern:\n",
      " [[2. 2. 2.]\n",
      " [2. 5. 2.]\n",
      " [2. 2. 2.]]\n",
      "Output for shifted cross pattern:\n",
      " [[0. 0. 1.]\n",
      " [0. 2. 2.]\n",
      " [1. 2. 5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv2d(x, w, b, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    x: input tensor in shape of (N, C_in, H_in, W_in)\n",
    "    w: weight tensor in shape of (C_out, C_in, H_k, W_k)\n",
    "    b: bias tensor in shape of (C_out,)\n",
    "    \"\"\"\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    C_out, _, H_k, W_k = w.shape\n",
    "\n",
    "    H_out = (H_in + 2 * padding - H_k) // stride + 1\n",
    "    W_out = (W_in + 2 * padding - W_k) // stride + 1\n",
    "\n",
    "    if padding > 0:\n",
    "        x_padded = np.pad(x, ((0,0), (0,0), (padding,padding), \n",
    "                              (padding,padding)), mode='constant')\n",
    "    else:\n",
    "        x_padded = x\n",
    "        \n",
    "    out = np.zeros((N, C_out, H_out, W_out))\n",
    "\n",
    "    for n in range(N):\n",
    "        for c_out in range(C_out):\n",
    "            for h_out in range(H_out):\n",
    "                for w_out in range(W_out):\n",
    "                    h_start = h_out * stride\n",
    "                    w_start = w_out * stride\n",
    "                    window = x_padded[n, :, h_start:h_start+H_k, w_start:w_start+W_k]\n",
    "                    \n",
    "                    conv_sum = np.sum(window * w[c_out, :, :, :])\n",
    "                    out[n, c_out, h_out, w_out] = conv_sum + b[c_out]\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #阶段一: 验证特征检测\n",
    "    # 1. 定义一个 5x5 的图像，中心有一个“十字”图案\n",
    "    image_centered = np.array([[\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 5, 5)\n",
    "\n",
    "    # 2. 设计一个 3x3 的卷积核，当它滑过输入图像时，如果它对应的图像区域与十字图案完全一致，它计算出的结果应该是最大的。\n",
    "    kernel_cross = np.array([[\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 3, 3)\n",
    "    bias_zero = np.array([0], dtype=np.float32)\n",
    "    \n",
    "    # 3. 执行卷积，观察输出\n",
    "    output_centered = conv2d(image_centered, kernel_cross, bias_zero, stride=1, padding=0)\n",
    "    print(\"Output for centered cross pattern:\\n\", output_centered[0, 0])\n",
    "\n",
    "\n",
    "    # 阶段二: 平移不变性\n",
    "    # 1. 创建一个新图像，将“十字”图案向右下方平移一格\n",
    "    image_centered = np.array([[\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 0]\n",
    "    ]], dtype=np.float32).reshape(1, 1, 5, 5)\n",
    "    # 2. 使用完全相同的卷积核进行卷积，并观察输出\n",
    "    \n",
    "    output_shifted = conv2d(image_centered, kernel_cross, bias_zero, stride=1, padding=0)\n",
    "    print(\"Output for shifted cross pattern:\\n\", output_shifted[0, 0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe867c",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "1. Locality: The convolution operation yields a feature map where each output value is influenced by a small, localized region of the input image. This is because the convolution kernel covers only a small part of the input at each iteration and computes a weighted sum and add a bias for that region. It has nothing to do with their absolute positions and other parts of the image.\n",
    "2. Translation Invariance: The convolution operation is translation invariant, because a translation of the feature(the cross's moving) in the input image will result in a corresponding translation of the feature(value '5.') in the output feature map, which is shown as the same value representing the same feature in different positions. It's just the same operation in another position.\n",
    "3. This means CNN can identify features effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b4ec0",
   "metadata": {},
   "source": [
    "#### c. Subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3270b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pooling Example:\n",
      "Original feature map 1:\n",
      " [[1. 2. 2. 0.]\n",
      " [4. 5. 3. 1.]\n",
      " [2. 3. 9. 9.]\n",
      " [0. 2. 9. 9.]]\n",
      "Original feature map 2:\n",
      " [[3. 2. 1. 4.]\n",
      " [2. 1. 3. 1.]\n",
      " [3. 9. 9. 1.]\n",
      " [1. 9. 9. 2.]]\n",
      "Pooled feature map 1:\n",
      " [[5. 3.]\n",
      " [3. 9.]]\n",
      "Pooled feature map 2:\n",
      " [[3. 4.]\n",
      " [9. 9.]]\n"
     ]
    }
   ],
   "source": [
    "def max_pool2d(x, kernel_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    x: input tensor of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    N, C, H_in, W_in = x.shape\n",
    "    H_out = (H_in - kernel_size) // stride + 1\n",
    "    W_out = (W_in - kernel_size) // stride + 1\n",
    "\n",
    "    out = np.zeros((N, C, H_out, W_out))\n",
    "\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for h_out in range(H_out):\n",
    "                for w_out in range(W_out):\n",
    "                    h_start = h_out * stride\n",
    "                    w_start = w_out * stride\n",
    "                    window = x[n, c, h_start:h_start+kernel_size, w_start:w_start+kernel_size]\n",
    "                    \n",
    "                    out[n, c, h_out, w_out] = np.max(window)\n",
    "    return out\n",
    "\n",
    "print(\"Max Pooling Example:\")\n",
    "# feature map with a simple pattern\n",
    "feature_map_1 = np.array([[\n",
    "    [1, 2, 2, 0],\n",
    "    [4, 5, 3, 1],\n",
    "    [2, 3, 9, 9],\n",
    "    [0, 2, 9, 9]\n",
    "]], dtype=np.float32).reshape(1, 1, 4, 4)\n",
    "\n",
    "feature_map_2 = np.array([[\n",
    "   [3, 2, 1, 4],\n",
    "   [2, 1, 3, 1],\n",
    "   [3, 9, 9, 1],\n",
    "   [1, 9, 9, 2],\n",
    "]], dtype=np.float32).reshape(1, 1, 4, 4)\n",
    "\n",
    "pooled_1 = max_pool2d(feature_map_1, kernel_size=2, stride=2)\n",
    "pooled_2 = max_pool2d(feature_map_2, kernel_size=2, stride=2)\n",
    "\n",
    "print(\"Original feature map 1:\\n\", feature_map_1[0, 0])\n",
    "print(\"Original feature map 2:\\n\", feature_map_2[0, 0])\n",
    "\n",
    "print(\"Pooled feature map 1:\\n\", pooled_1[0, 0])\n",
    "print(\"Pooled feature map 2:\\n\", pooled_2[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b03f3",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "- (1) we can see that the output feature map retains the most prominent features('9.') from the input feature map. The strongest feature is always retained after pooling, which means translation invariance is achieved to some extent: change the position of the feature won't change the feature value, so it still gets detected and retained.\n",
    "- (2) for this set up, the output of the pooling layer is halved in both height and width, effectively reducing the size of the feature map while retaining the most salient features. With a stride of s, the output dimensions will be reduced by a factor of s in both height and width, resulting a W*H/s^2 output feature map.\n",
    "- (3):\n",
    "    1. if adopting \"Average Pooling\", the output feature map will have averaged values in each pooling region, which may smooth out the features and make them less distinct. This could lead to loss of important details, especially if the features are not uniformly distributed.\n",
    "    2. when regarding feature selection, max pooling tends to effectively highlight the most prominent features, while average pooling may dilute them by averaging with less significant values. Just as their names suggest, \"max\" and \"average\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca0c15",
   "metadata": {},
   "source": [
    "#### d. Subtask 4\n",
    "##### Answer:\n",
    "- (1):\n",
    "    1. every element in the output probability vector is between 0 and 1 and they sum up to 1.0 .\n",
    "    2. These features makes softmax ideal for multi-class classification because its output can be interpreted as a probability distribution, which can be further interpreted as the model's confidence in each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a669b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: (1, 1, 28, 28)\n",
      "Logits shape: (1, 10) Probs shape: (1, 10)\n",
      "\n",
      "Logits: [ 0.00223343 -0.00416911  0.00250573  0.00267944  0.00646766  0.0015071\n",
      " -0.00022611  0.0042225   0.00018825  0.01054704]\n",
      "Probs: [0.09996308 0.09932511 0.09999031 0.10000768 0.10038725 0.0998905\n",
      " 0.09971752 0.10016211 0.09975885 0.1007976 ]\n",
      "\n",
      "Checksum logits sum: 0.025955935726856137\n",
      "Checksum probs sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "from array import array\n",
    "\n",
    "# (在此之前应有已实现的 conv2d, relu, max_pool2d, flatten,linear_layer函数)\n",
    "# 固定随机种子，保证权重初始化一致\n",
    "np.random.seed(114514)\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    实现Softmax函数。\n",
    "    logits: input tensor of shape (N, num_classes)\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "# --- MNIST 数据集读取函数 ---\n",
    "def read_images(filename):\n",
    "    \"\"\"\n",
    "    读取MNIST图像文件\n",
    "    参数:\n",
    "      filename: MNIST图像文件路径\n",
    "    返回:\n",
    "      images: 图像数组列表\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        \n",
    "        image_data = array(\"B\", file.read())\n",
    "        \n",
    "    images = []\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(rows, cols)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# =========================================================\n",
    "# ===== 任务：请根据下面的规约，在此处实现 TinyCNN_for_MNIST 类 =====\n",
    "# =========================================================\n",
    "#\n",
    "# --- 模型架构规约 ---\n",
    "# 1. 构造函数 `__init__(self)`:\n",
    "# 架构固定：Conv(1->4, k=3, stride=1, pad=1) -> ReLU -> MaxPool(2x2, s=2) -> Flatten -> Linear(->10 类)\n",
    "# 2. 前向传播方法 `forward(self, x)`:\n",
    "#    - 接收一个形状为 (N, 1, 28, 28) 的张量 x。\n",
    "#    - 按照以下顺序依次调用你实现的算子：\n",
    "#      Conv2d -> ReLU -> MaxPool2d -> Flatten -> Linear -> Softmax\n",
    "#    - 返回最终的 logits (Linear层输出) 和 probs (Softmax层输出)。\n",
    "\n",
    "class TinyCNN_for_MNIST:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.conv_w = np.random.randn(4, 1, 3, 3) * 0.01\n",
    "        self.conv_b = np.zeros(4)\n",
    "\n",
    "        self.lr_w = np.random.randn(784, 10) * 0.01\n",
    "        self.lr_b = np.zeros(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = conv2d(x, self.conv_w, self.conv_b,padding=1)\n",
    "        x = relu(x)\n",
    "        x = max_pool2d(x, 2, 2)\n",
    "        x = flatten(x)\n",
    "        logits = linear_layer(x, self.lr_w, self.lr_b)\n",
    "        probs = softmax(logits)\n",
    "        return logits, probs\n",
    "\n",
    "\n",
    "# --- 测试脚本 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 设置 MNIST 测试集文件路径\n",
    "    # !! 请将此路径修改为你自己的文件路径\n",
    "    mnist_test_file = '../mnist/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "\n",
    "    if not os.path.exists(mnist_test_file):\n",
    "        print(f\"错误：找不到 MNIST 测试集文件 '{mnist_test_file}'\")\n",
    "    else:\n",
    "        # 2. 加载所有测试图像\n",
    "        test_images = read_images(mnist_test_file)\n",
    "        # 3. 选取第一张图像作为测试输入\n",
    "        first_test_image = test_images[0]\n",
    "        # 4. 预处理图像\n",
    "        input_tensor = (first_test_image.astype(np.float32) / 255.0 - 0.5) * 2.0\n",
    "        input_tensor = np.expand_dims(input_tensor, axis=(0, 1))\n",
    "        # 5. 实例化模型并执行前向传播\n",
    "        model = TinyCNN_for_MNIST()\n",
    "        logits, probs = model.forward(input_tensor)\n",
    "\n",
    "        print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "        print(\"Logits shape:\", logits.shape, \"Probs shape:\", probs.shape)\n",
    "        np.set_printoptions(precision=8, suppress=False)\n",
    "        print(\"\\nLogits:\", logits[0])\n",
    "        print(\"Probs:\", probs[0])\n",
    "        print(\"\\nChecksum logits sum:\", float(np.sum(logits)))\n",
    "        print(\"Checksum probs sum:\", float(np.sum(probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03182861",
   "metadata": {},
   "source": [
    "##### Assessment:\n",
    "1. each prob value is close to 0.1, which is expected for a un-trained model\n",
    "2. they sum up to 1.0, which means softmax is working correctly\n",
    "3. the logits are small and close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe13a73",
   "metadata": {},
   "source": [
    "#### e. Subtask 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c8c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Logits: [[ 0.00353601 -0.00597512 -0.01245221  0.00474773 -0.00172211  0.00710951\n",
      "   0.00555682 -0.00088172  0.00028607 -0.00289982]]\n",
      "Torch Logits: [[ 0.00353601 -0.00597512 -0.01245222  0.00474773 -0.00172211  0.00710952\n",
      "   0.00555682 -0.00088171  0.00028607 -0.00289982]]\n",
      "\n",
      "Numpy Probs: [[0.10037968 0.09942948 0.09878755 0.10050138 0.09985325 0.10073903\n",
      "  0.10058273 0.0999372  0.10005398 0.09973572]]\n",
      "Torch Probs: [[0.10037968 0.09942947 0.09878755 0.10050138 0.09985325 0.10073902\n",
      "  0.10058273 0.09993721 0.10005397 0.09973572]]\n",
      "\n",
      "Are the outputs of the two models numerically close? True\n"
     ]
    }
   ],
   "source": [
    "# Subtask 5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PyTorch_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_CNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(4 * 14 * 14, 10)  # 4 channels, 14x14 after pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        return logits, probs\n",
    "    \n",
    "pytorch_model = PyTorch_CNN()\n",
    "numpy_model = TinyCNN_for_MNIST()\n",
    "\n",
    "# weight transfer\n",
    "with torch.no_grad():\n",
    "    conv_w_torch =  torch.from_numpy(numpy_model.conv_w).float()\n",
    "    conv_b_torch =  torch.from_numpy(numpy_model.conv_b).float()\n",
    "    fc_w_torch =  torch.from_numpy(numpy_model.lr_w.T).float()\n",
    "    fc_b_torch =  torch.from_numpy(numpy_model.lr_b).float()\n",
    "\n",
    "    pytorch_model.conv.weight.data = conv_w_torch\n",
    "    pytorch_model.conv.bias.data = conv_b_torch\n",
    "    pytorch_model.fc.weight.data = fc_w_torch\n",
    "    pytorch_model.fc.bias.data = fc_b_torch\n",
    "    \n",
    "mnist_test_file = '../mnist/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "if not os.path.exists(mnist_test_file):\n",
    "    print(f\"Error: '{mnist_test_file}' not found\")\n",
    "else:\n",
    "    test_images = read_images(mnist_test_file)\n",
    "    first_test_image = test_images[0]\n",
    "    \n",
    "    numpy_input = (first_test_image.astype(np.float32) / 255.0 - 0.5) * 2.0\n",
    "    numpy_input = np.expand_dims(numpy_input, axis=(0, 1))\n",
    "    \n",
    "    torch_input = torch.from_numpy(numpy_input)\n",
    "    \n",
    "    numpy_logits, numpy_probs = numpy_model.forward(numpy_input)\n",
    "    torch_logits, torch_probs = pytorch_model.forward(torch_input)\n",
    "    \n",
    "    print(\"Numpy Logits:\", numpy_logits)\n",
    "    print(\"Torch Logits:\", torch_logits.detach().numpy())\n",
    "    \n",
    "    print(\"\\nNumpy Probs:\", numpy_probs)\n",
    "    print(\"Torch Probs:\", torch_probs.detach().numpy())\n",
    "\n",
    "    are_close = np.allclose(numpy_logits, torch_logits.detach().numpy(), atol=1e-6)\n",
    "    print(f\"\\nAre the outputs of the two models numerically close? {are_close}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e0bd4",
   "metadata": {},
   "source": [
    "#### --- 4. 完成分析报告(mostly learned from AI) ---\n",
    "\n",
    "1. 功能性对比 ：除了我们实现的版本，PyTorch 的 nn.Conv2d 还提供了哪些我们的 NumPy 版本不具备的进阶功能或参数？\n",
    "    - groups: This parameter allows for grouped convolutions, where input channels are split into groups and each group is convolved with its own set of filters. When groups equals in_channels, it becomes a depthwise convolution.\n",
    "    - dilation: This enables dilated or \"atrous\" convolution, which increases the receptive field of the kernel without increasing the number of parameters by inserting gaps between kernel elements.\n",
    "    - Automatic Backend Optimization: It can use highly optimized libraries like cuDNN (on GPU) or MKL (on CPU) under the hood.\n",
    "    - Data Type Handling: It seamlessly handles various data types (e.g., float16, float32, bfloat16).\n",
    "\n",
    "2. 性能对比：为何 PyTorch 的运算速度会比我们手写的 Python for 循环快几个数量级？请从底层实现语言、并行计算策略、以及可能的算法优化等角度进行分析。\n",
    "    - **Underlying Language**: Our implementation uses nested Python for loops while PyTorch's core operations are implemented in highly optimized, compiled C++ and CUDA code, which executes directly on the hardware and is much faster.\n",
    "    - **Parallel Computation**: Our for loop implementation is inherently sequential. PyTorch is designed for parallelism. Parallel computation yields significant speedups, especially on GPUs.\n",
    "    - **Optimized Algorithms**: For convolutions, our naive implementation has a high time complexity. PyTorch (via libraries like cuDNN) can use much faster algorithms like Winograd or FFT-based convolution, which are mathematically equivalent but computationally far more efficient, especially for certain kernel sizes.\n",
    "\n",
    "3. 核心优势对比 ：在你看来，使用 PyTorch 这类框架相对于从零手写，其最根本、最无法替代的优势是什么？\n",
    "    The most fundamental and irreplaceable advantage is **Automatic Differentiation (Autograd)**.\n",
    "    - While we have implemented the forward pass, we haven't touched the backward pass (backpropagation), which is necessary for training. Manually deriving and implementing the gradients for every layer is incredibly complex, tedious, and error-prone. PyTorch's autograd engine automatically tracks all operations on tensors. When we call .backward() on a final loss value, it automatically computes the gradients of that loss with respect to all model parameters. This single feature abstracts away the most difficult part of building and training neural networks, allowing developers to focus on model architecture and experimentation, which dramatically accelerates research and development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
